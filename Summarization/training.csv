US8731929B2;System for responding to natural language speech utterance, has parser that determines context for keywords in utterance such that parser selects domain agents based on determined meaning The system has an agent architecture that has several domain agents that receive process and respond to request associated with a respective context. A parser (118) determines a context for keywords in the utterance and determines the meaning of utterance. The parser selects the domain agents based on determined meaning. An event manager (100) coordinates the interaction between parser and agent architecture. System for responding to natural language speech utterance, for use with portable computer and personal digital assistant (PDA). The parser determines the context for keywords in the utterance and determines the meaning of utterance so that parser selects the domain agents reliably based on determined meaning. Hence a complete speech-based natural language query and response environment can be provided efficiently. The user can submit the natural language speech questions and commands reliably. The drawing shows a schematic view of the system.100Event manager118Parser120Speech recognition engine124Text to speech engine136Internet A system for processing natural language utterances, comprising: a computing device having access to a plurality of domain agents associated with a plurality of different domains, and programmed to execute one or more computer program instructions which, when executed, cause the computing device to: receive a first natural language utterance; determine that the first natural language utterance contains one or more words that were unrecognized or incorrectly recognized in response to a recognition associated with the first natural language utterance having a confidence level below a predetermined value; obtain a phonetic alphabet spelling associated with the one or more unrecognized or incorrectly recognized words in response to the determination; look up the one or more unrecognized or incorrectly recognized words in one or more dictionary and phrase tables based on the phonetic alphabet spelling; update the one or more dictionary and phrase tables based on a pronunciation associated with the one or more unrecognized or incorrectly recognized words; receive a second natural language utterance that comprises a question; generate a digitized speech signal from the second natural language utterance; recognize one or more words in the second natural language utterance based on a pronunciation associated with the one or more words using the one or more dictionary and phrase tables; tag the one or more words in the second natural language utterance with a user identity determined from voice characteristics associated with the digitized speech signal and one or more user profiles; determine a context of the question in the second natural language utterance; select one of the plurality of domain agents based on the context of the question; generate a request associated with the second natural language utterance based on the one or more words in the second natural language utterance and a grammar used by the selected domain agent, wherein the request includes the question; invoke the selected domain agent to cause the selected domain agent to process the request; and receive a response to the request from the selected domain agent.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160188292A1;Method of interpreting natural language inputs based on storage of inputs, involves reprocessing natural language input obtained from storage to determine second interpretation of natural language input The method involves obtaining (402) a natural language input of a user by computer system. The natural language input is initially obtained through an input mode. The natural language input is processed (404) to determine a first interpretation of the natural language input. The natural language input is stored (406) based on a data format associated with the input mode. The natural language input is obtained (408) from storage. The natural language input obtained from storage is reprocessed (410) to determine a second interpretation of the natural language input. An INDEPENDENT CLAIM is also included for a system of interpreting natural language inputs based on storage of inputs. Method of interpreting natural language inputs based on storage of inputs. The audio file can be obtained from the cache and processed by the speech recognition engine to determine one or more reinterpretations of the utterance stored in the audio file in the event that the user input is to be reprocessed. The search can be performed, and the results of the search can be provided for presentation to the user if the selected interpretation indicates that the user provided the input to search for particular information. The drawing shows a flow diagram illustrating the method of interpreting natural language inputs based on storage of the inputs. 402Step for obtaining the natural language input of user404Step for processing the natural language input to determine first interpretation of the natural language input406Step for storing the natural language input408Step for obtaining the natural language input from storage410Step for reprocessing the natural language input obtained from storage to determine second interpretation of the natural language input A method of interpreting natural language inputs based on storage of the inputs, the method being implemented on a computer system that includes one or more physical processors executing computer program instructions which, when executed by the one or more physical processors, perform the method, the method comprising: obtaining, by the computer system, a natural language input of a user, wherein the natural language input is initially obtained via an input mode; processing, by the computer system, the natural language input to determine a first interpretation of the natural language input; storing, by the computer system, the natural language input based on a data format associated with the input mode; obtaining, by the computer system, the natural language input from storage; and reprocessing, by the computer system, the natural language input obtained from storage to determine a second interpretation of the natural language input.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9607102B2;Method for switching task in conversational dialogue software application installed in e.g. mobile device, involves changing task rule for corresponding task from rules-based model to optimized statistical based task switching model The method involves optimizing a statistical task switching model based on first quality metric. Second quality metric is calculated based on whether a portion of identified first candidate tasks correspond to secondary tasks. A determination is made to check whether one of the rules-based tasks switching rules is modified based on whether the second quality metric satisfies threshold quantity. A task switching rule is changed for a corresponding candidate task from a rules-based model to an optimized statistical based task model when the second quality metric satisfies the threshold quantity. INDEPENDENT CLAIMS are also included for the following:a system for switching a task in a conversational dialogue software application installed in a computing devicea non-transitory computer-readable storage medium comprising a set of instructions for switching a task in a conversational dialogue software application installed in a computing device. Method for switching a task in a conversational dialogue software application installed in a computing device e.g. mobile device and laptop, through a network. Uses include but are not limited to private intranets, corporate networks, LANs, wireless networks and personal networks (PAN). The method enables providing flexibility in managing task interruptions by allowing or not allowing rules or restrictions to be implemented. The method enables performing functionality of data server software to operations or decisions made automatically based on the rules coded into a control logic. The drawing shows a flow diagram illustrating a method for switching a task in a conversational dialogue software application installed in a computing device. 302Selector304Intent312From account314To account316Amount A method comprising: receiving, by a computing device comprising a natural language understanding automatic speech recognition computing engine device, a first natural language input comprising one or more words; activating, by the natural language understanding automatic speech recognition computing engine device, a first task based on the first natural language input, wherein the first task is associated with one or more first task agents configured for retrieving information associated with the first task; prompting, by the natural language understanding automatic speech recognition computing engine device and via a first of the one or more first task agents, for a subsequent natural language input based on a first transcription of the first natural language input and based on a first intent associated with the first task; receiving, by the natural language understanding automatic speech recognition computing engine device and while the first task is activated, a second natural language input comprising one or more words; responsive to determining, by the natural language understanding automatic speech recognition computing engine device, that a task activation switching parameter associated with the first task is not a false value and that a second intent associated with the second natural language input is different from the first intent associated with the first task, determining, by the natural language understanding automatic speech recognition computing engine device: one or more candidate second tasks that are capable of being activated based on one or more task switching rules that identify one or more tasks that are allowed to interrupt the activated first task, wherein an interrupted task is arranged in a task stack memory component of the computing device; and one or more candidate third tasks that are incapable of being activated based on the one or more task switching rules;  activating, by the natural language understanding automatic speech recognition computing engine device, one of the one or more candidate second tasks, wherein the activated candidate second task is associated with one or more second task agents configured for retrieving information associated with the activated candidate second task; responsive to satisfying the one or more second task agents with the first natural language input, the second natural language input, or an additional natural language input, performing, by the computing device, an action associated with the activated second candidate task; determining, by the natural language understanding automatic speech recognition computing engine device, whether the second natural language input satisfies the one or more first task agents associated with the first task; performing, by the computing device, an action associated with the first task responsive to the second natural language input satisfying the one or more first task agents; and prompting, by the natural language understanding automatic speech recognition computing engine device and via one of the one or more first task agents, for a second subsequent natural language input based on the first transcription of the first natural language input responsive to the second natural language input not satisfying the one or more first task agents.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US8886536B2;Computer-implemented method of obtaining conversationally- related promotional content, involves determining promotional content by physical processors based on natural language utterances, and presenting promotional content to user The method involves receiving first natural language utterance by multiple physical processors, and providing a response to first natural language utterance by physical processors. A second natural language utterance relating to the first natural language utterance is received by the physical processors. A promotional content is determined by the physical processors based on the first natural language utterance and the second natural language utterance. The promotional content is presented to a user by the physical processors. An INDEPENDENT CLAIM is included for a system for obtaining conversationally-related promotional content. Computer-implemented method of obtaining conversationally- related promotional content. The conversationally- related promotional content can be obtained efficiently. The advertisements can be presented based on natural language processing of voice-based input. The user interaction with the system for obtaining conversationally-related promotional content can be improved. The advertisement selection performance using the advertisement selection module can be improved. The drawing shows a block diagram of the system for implementing a voice user interface. 100System for implementing voice user interface105Input device110Automatic speech recognizer120Conversational language processor135Agents A computer-implemented method of providing promotional content related to one or more natural language utterances and/or responses, the method being implemented by a computer system that includes one or more physical processors executing one or more computer program instructions which, when executed, perform the method, the method comprising: receiving, at the one or more physical processors, a first natural language utterance; providing, by the one or more physical processors, a response to the first natural language utterance; receiving, at the one or more physical processors, a second natural language utterance relating to the first natural language utterance; performing, by the one or more physical processors, speech recognition to recognize one or more words of the second natural language utterance; determining, by the one or more physical processors, domain information for the one or more recognized words based on the first natural language utterance; processing, by the one or more physical processors, based on the domain information, the one or more recognized words to determine an interpretation of the second natural language utterance, wherein processing the one or more recognized words comprises: providing the one or more recognized words to a first domain agent associated with a first domain and a second domain agent associated with a second domain; obtaining a first interpretation of the second natural language utterance from the first domain agent; obtaining a second interpretation of the second natural language utterance from the second domain agent; and determining the interpretation based on one or more of the first interpretation or the second interpretation;  determining, by the one or more physical processors, promotional content based on the interpretation; and presenting, by the one or more physical processors, the promotional content to a user.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160260430A1;Computer-implemented method for converting natural language input into command input for target device, involves performing input transformation on portion of cleaned natural language input The method involves converting an audio input into text with a natural language input. A portion of the natural language input is intended for a target device that requires commands to be properly formed for the target device to be executed. A natural language processing (NLP) input cleaning is performed (610) on the natural language input. An input transformation is performed (615) on a portion of the cleaned natural language input to convert the cleaned natural language input one or more properly formed commands that are target-specific for the target device. INDEPENDENT CLAIMS are included for the following:a natural language processing (NLP) system for converting a natural language input into a target-specific command input for a target device; anda non-transitory computer-readable medium or media storing program for converting a natural language input into a command input for a target device. Computer-implemented method for converting a natural language input into a command input for a target device such as personal computer e.g. desktop or laptop, tablet computer, mobile device e.g. personal digital assistant (PDA) or smart phone, server e.g. blade server or rack server and a network storage device used in financial transaction processing, airline reservations, enterprise data storage and global communication in business, personal and other purpose. The network administration staff can be more familiar with certain vendors' configuration and maintenance requirements and suggest or buy those vendors' equipment. A user-friendly natural language interface can be provided by a single point of administration, management and monitoring across a network. The command line interface (CLI) definition files tend to be more accurate than statistical models. The system can use corrected changes whether audio or text to improve the learning process. An input can be sent to a set of devices by providing ease and efficiency for the user. The drawing shows a flow chart illustrating the method for performing a natural language processing (NLP) query look-up. 605Step for receiving natural language input610Step for performing natural language processing input cleaning615Step for performing query transformation620Step for forming proper commands625Step for receiving command results A computer-implemented method for converting a natural language input into a command input for a target device comprising: converting an audio input into text comprising a natural language input, at least a portion of the natural language input being intended for a target device that requires commands to be properly formed for the target device to be executed; performing natural language processing (NLP) input cleaning on the natural language input; and performing input transformation on at least a portion of the cleaned natural language input to convert the cleaned natural language input one or more properly formed commands that are target-specific for the target device.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170154628A1;Method for processing a natural language query, involves receiving the natural language query for the application, where natural language processor uses natural language modules to interpret the natural language query for the application The method involves receiving the natural language query for the application. The natural language processor uses natural language modules to interpret the natural language query for the application. A charge for processing the natural language query is calculated for each of the natural language module. The charge is determined in accordance with a pricing model defined for the natural language module. Method for processing a natural language query. The method involves receiving the natural language query for the application, and hence ensures simple and efficient processing method. The drawing shows a schematic representation of an assembling natural language recognition functionality. 401Integrator402Natural language module412Natural language query413Natural language module list414Natural language module selection A method for processing a natural language query, the method comprising: receiving the natural language query for the application; a natural language processor using one or more natural language modules to interpret the natural language query for the application; and for each of the one or more natural language modules: calculating a charge for processing the natural language query, the charge determined in accordance with a pricing model defined for the natural language module.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9805717B2;Computer-implemented method for converting natural language input into command input for target device, involves performing input transformation on portion of cleaned natural language input The method involves converting an audio input into text with a natural language input. A portion of the natural language input is intended for a target device that requires commands to be properly formed for the target device to be executed. A natural language processing (NLP) input cleaning is performed (610) on the natural language input. An input transformation is performed (615) on a portion of the cleaned natural language input to convert the cleaned natural language input one or more properly formed commands that are target-specific for the target device. INDEPENDENT CLAIMS are included for the following:a natural language processing (NLP) system for converting a natural language input into a target-specific command input for a target device; anda non-transitory computer-readable medium or media storing program for converting a natural language input into a command input for a target device. Computer-implemented method for converting a natural language input into a command input for a target device such as personal computer e.g. desktop or laptop, tablet computer, mobile device e.g. personal digital assistant (PDA) or smart phone, server e.g. blade server or rack server and a network storage device used in financial transaction processing, airline reservations, enterprise data storage and global communication in business, personal and other purpose. The network administration staff can be more familiar with certain vendors' configuration and maintenance requirements and suggest or buy those vendors' equipment. A user-friendly natural language interface can be provided by a single point of administration, management and monitoring across a network. The command line interface (CLI) definition files tend to be more accurate than statistical models. The system can use corrected changes whether audio or text to improve the learning process. An input can be sent to a set of devices by providing ease and efficiency for the user. The drawing shows a flow chart illustrating the method for performing a natural language processing (NLP) query look-up. 605Step for receiving natural language input610Step for performing natural language processing input cleaning615Step for performing query transformation620Step for forming proper commands625Step for receiving command results A computer-implemented method for converting a natural language input into a command input for a target device comprising: converting an audio input into text comprising a natural language input, at least a portion of the natural language input being intended for a target device from among a plurality of potential target devices that requires commands to be properly formed for the target device to be executed; performing natural language processing (NLP) input cleaning on the natural language input; and performing input transformation on at least a portion of the cleaned natural language input to convert the cleaned natural language input into one or more properly formed commands that are target-specific for the target device by performing the steps comprising: performing a lookup in a lemmatization database to convert at least some of the cleaned natural language input to a target-specific input that comprises target-specific vocabulary that corresponds to the target device; and using the target-specific input to perform a lookup in a command template database to obtain a target-specific command template match corresponding to the target-specific input that comprises target-specific vocabulary, the target-specific command template being used to create at least one of the one or more properly formed commands.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9305548B2;Method of processing natural language utterances, involves receiving second prediction of intent of user and determining intent of user based on first prediction and second prediction The method involves performing speech recognition to determine words of the natural language utterance. A first prediction of intent of the user is determined based on the words. The natural language utterance is transmitted to a second device. A second prediction of the intent of the user is received from the second device by the first device. The intent of the user is determined based on the first prediction and the second prediction. An INDEPENDENT CLAIM is included for a system for processing natural language utterances. Method of processing natural language utterances in integrated, multi-modal, multi-device natural language voice services environment. The information contained in the recognition grammars are dynamically optimized to improve a likelihood of a given utterance being recognized accurately. The additional knowledge is provided to a device having low intent determination reliability, even when knowledge results in redundancy in the environment, to ensure commensurate reliability of intent determination environment-wide. The drawing shows the block diagram of the multi-modal electronic device provided in an integrated, multi-device natural language voice services environment. 100Electronic device105Input mechanism110Automatic speech recognizer120Conversational language processor160Data repository A method of processing natural language utterances, the method being implemented by a first device that comprises one or more physical processors executing one or more computer program instructions which, when executed, perform the method, the method comprising: receiving, by the first device, a natural language utterance spoken by a user; performing, by the first device, speech recognition to determine one or more words of the natural language utterance; determining, by the first device, based on the one or more words, a first prediction of an intent of the user and a domain relating to the natural language utterance; determining, by the first device, processing capabilities associated with a plurality of devices, wherein the plurality of devices comprises a second device; selecting, by the first device, based on the processing capabilities associated with the second device and the domain, the second device to determine the second prediction; transmitting, by the first device, the natural language utterance to the second device based on the selection; receiving, from the second device by the first device, a second prediction of the intent of the user; and determining, by the first device, the intent of the user based on the first prediction and the second prediction.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN105427122A;Advertisement e.g. sponsored message, selecting and presenting method for e.g. marketing, involves selecting advertisement based on received request and retrieved information, and presenting selected advertisement to user The method involves receiving a voice-based input including a request, and retrieving information from a knowledge source based on the received request. An advertisement is selected based on the received request and the retrieved information. The selected advertisement is presented to a user, where the retrieved information is related to a content of a voice-based input, an action associated with the request, a location of the user, a user-specific profile and a global user profile e.g. demographics. An INDEPENDENT CLAIM is also included for a system for selecting and presenting advertisements in response to voice-based input that is received by a voice user interface. Method for selecting and presenting advertisements such as sponsored message, calls to action, purchase opportunity, and trial downloads, and other marketing communication in response to voice-based inputs received by a voice user interface having interface with a voice-enabled device such as a personal navigation device, personal digital assistant, media device, telematics device, personal computer, and a mobile phone for marketing and advertising. The method allows the user to select an advertisement, and interprets a subsequent voice-based input with shared knowledge of the advertisement that is related to the voice-based input, so that the advertisements enable the advertisers to market to consumers, thus improving the consumer interaction with the consumer electronics device. The drawing shows a block diagram of a system for implementing a voice user interface.105Input device110Automatic speech recognizer120Conversational language processor125Voice search engine130Context determination module of supply to one of one or more of nature language sentence/promotional content response to the computer implementation of the method, the method through computer system comprising executing one or more of computer program instructions of one of one or more of physical processor is executing, the number one or more of computer program instructions executing on time by carrying out the method, said method comprises: receiving the first language sentence to in the nature of one or more of physical processor, of the one or more one of physical processor providing the response to the first nature language utterance, receiving on the first language sentence in the nature of one or more physical processor of the position, of the one or more one of physical processor carrying out voice recognition, to identify the second language sentence and nature of one or more of words, from the one or more of the whole physical processor based on the first language sentence nature determining out the recognition of one or more words of the area information, from the analysis of one or more of physical processor based on the area information to identify out the one or more number of word processing and determining the second language sentence is nature, The corresponding outlet of the recognition of one or more words of process comprises: providing the one or more of the whole word identification out of and the first area relevant to the local agency and the first and the second area of the second relevant local agency, from the first local agency first parsing the second nature language sentence is obtained, from the second region the second agency obtaining second nature language analysis of speech, and based on the first or second analysis and the analysis of one or more of the determined parsing, by the one or more of physical processor based on the analysis of determining promotional content, and the one of two or more physical processor presentation of the promotional content to user.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20150142447A1;Method of processing natural language utterances, involves receiving second prediction of intent of user and determining intent of user based on first prediction and second prediction The method involves performing speech recognition to determine words of the natural language utterance. A first prediction of intent of the user is determined based on the words. The natural language utterance is transmitted to a second device. A second prediction of the intent of the user is received from the second device by the first device. The intent of the user is determined based on the first prediction and the second prediction. An INDEPENDENT CLAIM is included for a system for processing natural language utterances. Method of processing natural language utterances in integrated, multi-modal, multi-device natural language voice services environment. The information contained in the recognition grammars are dynamically optimized to improve a likelihood of a given utterance being recognized accurately. The additional knowledge is provided to a device having low intent determination reliability, even when knowledge results in redundancy in the environment, to ensure commensurate reliability of intent determination environment-wide. The drawing shows the block diagram of the multi-modal electronic device provided in an integrated, multi-device natural language voice services environment. 100Electronic device105Input mechanism110Automatic speech recognizer120Conversational language processor160Data repository 28. A method of processing natural language utterances, the method being implemented by a first device that comprises one or more physical processors executing one or more computer program instructions which, when executed, perform the method, the method comprising: receiving, by the first device, a natural language utterance spoken by a user; performing, by the first device, speech recognition to determine one or more words of the natural language utterance; determining, by the first device, based on the one or more words, a first prediction of an intent of the user; transmitting, by the first device, the natural language utterance to a second device; receiving, from the second device by the first device, a second prediction of the intent of the user; and determining, by the first device, the intent of the user based on the first prediction and the second prediction.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN107148554A;Navigation system providing user adaptive navigation directions using natural language interface in e.g. smartphone, has directions engine presenting user adaptive navigation directions by considering prior and current user behavior data The system (100) has a route engine generating a route from a first location to a desired destination using map information. A log engine (132) logs a current user-system interaction including current user behavior data. An adaptive directions engine generates and presents user adaptive navigation directions by considering prior user behavior data and the current user behavior data to determine a classification of the user input and selecting user adaptive navigation directions based on the user input and the classification of the user input. INDEPENDENT CLAIMS are also included for the following:a method for providing user adaptive navigation directionsa computer-readable medium comprising a set of instructions for executing a method for providing user adaptive navigation directions. Navigation system providing user adaptive navigation directions using a natural language interface in a mobile computing device e.g. laptop, tablet and smartphone. The system provides user adaptive natural language interface to improve user experience for user to provide responses that are better suited for more palatable, acceptable and satisfactory to the user. The drawing shows a schematic block diagram of a system for providing a user adaptive natural language interface. 100Navigation system106Audio output128Database132Log engine140Network interface A navigation system for providing user self-adapting navigation guide, comprising: an input analyzer analyzing user input to derive for guiding to a desired destination of request and deriving current user behavior data, map data, providing map information; routing engine, using the map information generating from the first position a route to the desired destination, a recording engine, recording the current user-system interaction, the current user-system interaction comprises the user behaviour data, and adaptive guidance engine. through considering the previous user behaviour data and current user behavior data to determine the type of the user input, and through based on the category of the user input and the user input to select a user self-adapting navigation directions, so as to generate and present the user self-adapting navigation directions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160092160A1;Navigation system providing user adaptive navigation directions using natural language interface in e.g. smartphone, has directions engine presenting user adaptive navigation directions by considering prior and current user behavior data The system (100) has a route engine generating a route from a first location to a desired destination using map information. A log engine (132) logs a current user-system interaction including current user behavior data. An adaptive directions engine generates and presents user adaptive navigation directions by considering prior user behavior data and the current user behavior data to determine a classification of the user input and selecting user adaptive navigation directions based on the user input and the classification of the user input. INDEPENDENT CLAIMS are also included for the following:a method for providing user adaptive navigation directionsa computer-readable medium comprising a set of instructions for executing a method for providing user adaptive navigation directions. Navigation system providing user adaptive navigation directions using a natural language interface in a mobile computing device e.g. laptop, tablet and smartphone. The system provides user adaptive natural language interface to improve user experience for user to provide responses that are better suited for more palatable, acceptable and satisfactory to the user. The drawing shows a schematic block diagram of a system for providing a user adaptive natural language interface. 100Navigation system106Audio output128Database132Log engine140Network interface A navigation system providing user adaptive navigation directions, comprising: an input analyzer to analyze user input to derive a request for directions to a desired destination and to derive current user behavior data; map data providing map information; a route engine to generate a route from a first location to the desired destination using the map information; a log engine to log a current user-system interaction, including current user behavior data; and an adaptive directions engine to generate and present user adaptive navigation directions, by considering prior user behavior data and the current user behavior data to determine a classification of the user input and selecting user adaptive navigation directions based on the user input and the classification of the user input.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160372112A1;System for managing interactions between user and applications, has processor is configured to parse voice command, and parsing including processing natural language associated with voice command The system (300) has a processor is configured to receive a command (120) including a voice command from a user (130). The processor also derives the one or more key words from the voice command. The processor also selects an executing device for executing the voice command based on the one or more key words. The processor also directs the voice command to the executing device to execute the voice command. The processor also parses the voice command, and the parsing including processing a natural language associated with the voice command. An INDEPENDENT CLAIM is also included for a method for managing interactions between a user and applications. System for managing interactions between user and applications. The command execution confirmation is provided to the user by reproducing the command execution confirmation by the speech. The user can manage the bi-directional communication with the applications and other users associated with the applications. The systems is accessible to outside users who deploy applications within the computing infrastructure to obtain the benefit of large computational or storage resources. The drawing shows a schematic block diagram of a system. 110Network120Command130User140User interface300System A system for managing interactions between a user and applications, the system comprising: a processor operable to: receive a command from a user, the command including a voice command; derive one or more key words from the voice command, the one or more key words being associated with one or more executing devices, wherein the one or more executing devices are associated with one or more applications; select, based on the one or more key words, an executing device for executing the voice command; and direct the voice command to the executing device to execute the voice command; and  a parser in communication with the processor and operable to: parse the voice command, the parsing including processing a natural language associated with the voice command.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170083285A1;System for facilitating selection of device for providing response to provide communication between speech interface devices, has arbiter for determining that first timestamp is greater than second timestamp, and aborting pipeline instance The system (100) has a response dispatcher (122) positioned in speech processing pipeline instance (116a) after natural language understanding (NLU) component (120). The response dispatcher specifies a speech response to speech utterance. A source arbiter (124a) is positioned in the instance before an automatic speech recognition (ASR) component (118). The source arbiter determines that amount of time represented by difference between first timestamp and second timestamp is less than threshold, determines that the first timestamp is greater than the second timestamp, and aborts the instance. An INDEPENDENT CLAIM is also included for a method for facilitating device selection for providing a response. System for facilitating selection of device for providing a response to provide communication between speech interface devices to interact with a user by speech. Uses include but are not limited to personal computers, smartphones, tablet devices, media devices, entertainment devices, industrial systems and voice-based assistants to provide communication between wireless or Wi-Fi network communications interface, an Ethernet communications interface, a cellular network communications interface, and Bluetooth communications interface. The system facilitates selection of device for providing a response to provide communication between speech interface devices to interact with the user by speech in an effective manner. The drawing shows a schematic block diagram of a speech-based system for receiving user utterances from multiple speech interface devices. 100System for facilitating selection of device for providing response to provide communication between speech interface devices116a, 116bSpeech processing pipeline instances118ASR component120NLU component122Response dispatcher124a-124cSource arbiters A system, comprising; a first speech processing pipeline instance that receives a first audio signal from a first speech interface device, the first audio signal representing a speech utterance, the first speech processing pipeline instance also receiving a first timestamp indicating a first time at which a wakeword was detected by the first speech interface device; a second speech processing pipeline instance that receives a second audio signal from a second speech interface device, the second audio signal representing the speech utterance, the second speech processing pipeline also receiving a second timestamp indicating a second time at which the wakeword was detected by the second speech interface device; the first speech processing pipeline instance having a series of processing components comprising: an automatic speech recognition (ASR) component configured to analyze the first audio signal to determine words of the speech utterance; a natural language understanding (NLU) component positioned in the first speech processing pipeline instance after the ASR component, the NLU component being configured to analyze the words of the speech utterance to determine an intent expressed by the speech utterance; a response dispatcher positioned in the first speech processing pipeline instance after the NLU component, the response dispatcher being configured to specify a speech response to the speech utterance; a first source arbiter positioned in the first speech processing pipeline instance before the ASR component, the first source arbiter being configured to determine (a) that an amount of time represented by a difference between the first timestamp and the second timestamp is less than a threshold; (b) to determine that the first timestamp is greater than the second timestamp; and (c) to abort the first speech processing pipeline instance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170221484A1;User interface device for use with e.g. cell phone of e.g. TV used by elderly or lonely person for social interaction, has audio visual output presenting anthropomorphic object controlled by automated processor The device i.e. personal interface device has an audio visual information input to receive information sufficient to determine a topic of interest to a user and a query by a user depending on the received audio visual information. An audio visual output presents an anthropomorphic object controlled by an automated processor for conveying information of interest to the user depending on the determined topic of interest and the query with an audio visual telecommunication interface. An automated processor controls the anthropomorphic object with associated anthropomorphic mood. INDEPENDENT CLAIMS are also included for the following:a user interface system comprising an interactive interfacea method for using a User interface device. User interface device i.e. personal interface device for use with electronic system such as cell phone, smart phone i.e.    Apple iPhone  (RTM: Internet-connected multimedia smartphone), laptop computer and desktop computer of consumer electronics device such as TV, smart clock radio i.e.    Sony Dash  (RTM: fun device) and special purpose robot used by an elderly or lonely person for social interaction. Can also be used for electronic system such as    Blackberry  (RTM: handheld wireless electronic-mail device) smart phone, personal digital assistant (PDA) such as    Apple iPad  (RTM: tablet computer),    Apple iPod  (RTM: Portable media player) and    Amazon Kindle  (RTM: electronic-book reader) of consumer electronics device such as automobiles, set top box, stereo equipment, kitchen appliance, thermostats and heating, ventilation, and air conditioning (HVAC) equipment and laundry appliance. The device enables effectively detecting the user's facial features without discomfort to user. The drawing shows a front view of a cell phone.110Microphone120Camera130Speaker140Display150Image An electronic device comprising: at least one microphone for receiving a portion of a speech by a user, sufficient to determine a content of the speech including at least one of a topic of interest to a user, a mood of the user, and a query by the user; at least one processor; a system memory coupled with the processor; a network connection configured to communicate information representing the content of the speech to a remote server operating a speech recognition software, and to receive a response from the remote server; and at least one speaker configured to verbally present said response from the remote server, conveying information responsive to the topic of interest to the user, the mood of the user, and the query by the user.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170270916A1;Method for processing voice inputs, involves receiving voice input from user, and analyzing received voice input using processor to determine whether voice input includes one of human-intended content or machine-intended content The method involves receiving a voice input from a user, and analyzing the received voice input using a processor to determine whether the voice input includes one of human-intended content or machine-intended content. A human recipient is identified within the voice input for the human-intended content. A message is sent to the identified human recipient, where the message includes the human-intended content in an audio form. INDEPENDENT CLAIMS are included for the following:a voice input system;a vehicle with processor; anda computer program product for processing voice inputs. Method for processing voice inputs for a vehicle (Claimed). The method involves receiving a voice input from a user, and analyzing the received voice input using a processor to determine whether the voice input includes one of human-intended content or machine-intended content, and thus ensures simple and efficient processing voice inputs. The drawing shows a flow chart of a method. A method of processing voice inputs, the method comprising: receiving a voice input from a user; analyzing, using a processor, the received voice input to determine whether the voice input includes at least one of human-intended content or machine-intended content; responsive to determining that the voice input includes human-intended content: identifying within the voice input a human recipient for the human-intended content; and sending a message to the identified human recipient, the message including the human-intended content in an audio form; and responsive to determining that the voice input includes machine-intended content: identifying within the voice input a machine recipient for the machine-intended content; and sending a message including the machine-intended content to the identified machine recipient to implement the machine-intended content.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9406078B2;Method for processing natural language utterances e.g. microphone, involves providing selected advertisement through output device that is coupled to conversational language processor and such that response is obtained and provided The method involves providing natural language utterance as an input to a speech recognition engine. The words or phrases are provided as an input to conversational language processor (120). The utterance is interpreted at processor based on recognized words or phrases. The request is determined based on interpretation. A context is determined based on recognized words or phrases. An advertisement is selected based on determined context. The selected advertisement is provided through an output device. A response is obtained to request. The response is provided to request. Method for processing natural language utterances (claimed) e.g. microphone based on advertisements such as various contexts or topic concepts e.g. semantic indicators for music concept includes words such as music, tunes and songs, target demographics e.g. preferred audience, marketing criteria or prices for insertion e.g. dynamic or static pricing based on various marketing criteria or other information. The tracking of user interaction with advertisements can be used to build user-specific and/or global statistical profiles that map or cluster advertisements to topics, semantic indicators, contexts and concepts based on user behavior, demographics, targeting constraints, content of advertisements, content of requests, actions associated with requests or other statistically relevant information. The advertisements can be used to enable advertisers to market to consumers, while improving the consumers interaction with a device. The drawing shows a block diagram of system for implementing a voice user interface. 105Implementation input device110Automatic speech recognizer120Conversational language processor140Data repository180Output A method for processing natural language utterances that include requests and selecting and presenting advertisements based thereon, the method being implemented by one or more physical processors programmed with computer program instructions, which when executed cause the one or more physical processors to perform the method, the computer program instructions comprising at least a conversational language processor configured to interpret a natural language utterance, which relates to a request, based on words or phrases recognized from the natural language utterance, the method comprising: in response to receiving the natural language utterance, providing the natural language utterance as an input to a speech recognition engine; in response to receiving the words or phrases, recognized from the natural language utterance, as an output of the speech recognition engine, providing the words or phrases as an input to the conversational language processor; interpreting the natural language utterance, at the conversational language processor, based on the recognized words or phrases; determining the request based on the interpretation of the natural language utterance; determining a context for the natural language utterance based at least on the recognized words or phrases; selecting an advertisement based at least on the determined context; providing the selected advertisement via an output device coupled to the conversational language processor; obtaining a response to the request; providing the response to the request; in response to receiving a second natural language utterance, providing the second natural language utterance as an input to the speech recognition engine; in response to receiving a second set of words or phrases, recognized from the second natural language utterance, as a second output of the speech recognition engine, providing the second set of words or phrases as a second input to the conversational language processor; interpreting the second natural language utterance at the conversational language processor based on the recognized second set of words or phrases and the determined context; and determining a second request, related to the advertisement, based on the interpretation of the second natural language utterance; and obtaining and providing a second response to the second request.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20130159001A1;Method for satisfying specified intents based on e.g. multi-modal requests on tablet, involves monitoring audio signals received through audio interface for speech requests from user to be processed using speech understanding functionality The method (200) involves determining that entities in visual content are selected in accordance with an explicit scoping command from a user (202), where the content is displayed on a display of a processing system. The speech understanding functionality of the system is automatically activated (204) by using a processor of the system when the entities are selected. Audio signals received through an audio interface of the processing system are automatically monitored (206) for speech requests from the user to be processed using the understanding functionality when the entities are selected. INDEPENDENT CLAIMS are also included for the following:a processing system comprising a displaya computer program product comprising a set of instructions to perform a method for satisfying specified intents. Method for satisfying specified intents based on multi-modal requests on a processing system e.g. tablet, mobile phone and desktop computer. Uses include but are not limited to other requests e.g. speech requests, text commands, tactile commands and visual commands. The method enables quickly satisfying the specified intents based on the multi-modal requests for the user in a simple manner without distracting the user from the task so as to achieve quicker completion of tasks. The drawing shows a flowchart illustrating a method for satisfying specified intents based on multimodal requests.200Method for satisfying specified intents based on multi-modal requests202Step for determining that entities in visual content are selected in accordance with explicit scoping command from user204Step for automatically activating speech understanding functionality of processing system206Step for automatically monitoring audio signals received through audio interface of processing system208Step for automatically monitoring tactile interface of processing system What is claimed is:1 . A method comprising:  determining that one or more entities in visual content, which is displayed on a display of a processing system, are selected in accordance with an explicit scoping command from a user, the explicit scoping command identifying the one or more entities in the visual content to explicitly indicate a scope of an interaction between the user and the processing system;automatically activating speech understanding functionality of the processing system, using at least one processor of the processing system, in response to determining that the one or more entities are selected, the speech understanding functionality enabling the processing system to understand natural language requests; andautomatically monitoring audio signals received via an audio interface of the processing system for speech requests from the user to be processed using the speech understanding functionality in response to determining that the one or more entities are selected.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9542949B2;Method for satisfying specified intents based on multimodal requests, involves monitoring audio signals received through audio interface of processing system for speech requests from user of processing system to be processed The method involves determining that a camera of a processing system (100) is pointed at objects or scene. The speech understanding functionality of processing system is turned on, where speech understanding functionality enabling processing system to understand natural language requests. The audio signals received through an audio interface of processing system are automatically monitored for speech requests from a user of processing system to be processed using speech understanding functionality, in response to determining that camera is pointed at objects or scene. An INDEPENDENT CLAIM is included for a processing system. Method for satisfying specified intents based on multimodal requests. Since the audio signals that are received via an audio interface of the processing system are automatically monitored, the speech requests are provided to the speech understanding logic for processing. Thus, the processor-based system is enabled to automatically activate the speech understanding functionality of the processor-based system in response to determination that the one or more entities are selected. The drawing shows a schematic block diagram of the processing system. 100Processing system102Speaker104Display106Multimodal logic108Instance multimodal sensor A method comprising: determining that a camera of a processing system is pointed at one or more objects or a scene; turning on speech understanding functionality of the processing system, using at least one processor of the processing system, in response to determining that the camera is pointed at the one or more objects or the scene, the speech understanding functionality enabling the processing system to understand natural language requests; and automatically monitoring audio signals received via an audio interface of the processing system for speech requests from a user of the processing system to be processed using the speech understanding functionality in response to determining that the camera is pointed at the one or more objects or the scene.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9171541B2;Method for hybrid processing in natural language voice services environment, involves resolving multi-modal interaction at electronic device based on information contained in multiple messages received from virtual router The method involves detecting multi-modal interaction at an electronic device e.g. mobile phone. Multiple messages containing information related to the multi-modal interaction is communicated to a virtual router in communication with the electronic device. Multiple messages containing information related to intent of the multi-modal interaction is received by the electronic device. The multi-modal interaction at the electronic device is resolved based on the information contained in multiple messages received from the virtual router. INDEPENDENT CLAIMS are also included for the following:an electronic device for hybrid processing in a natural language voice services environmenta virtual router for hybrid processing in a natural language voice services environment. Method for hybrid processing in natural language voice services environment using an electronic device e.g. microphone, personal navigation device, mobile phone, Voice over Internet Protocol (VoIP) node, personal computer, media device, embedded device and server, and a virtual router (all claimed). The method enables performing hybrid processing in the natural language voice services environment using the electronic device and/or the virtual router without degrading performance in an efficient manner. The drawing shows a block diagram of a voice-enabled device for hybrid processing in a natural language voice services environment.100Voice-enabled device112Input devices120Automatic speech recognizer130Conversational language processor134Applications A method of natural language utterance processing, the method being implemented in a computer system that includes one or more physical processors executing one or more computer program instructions which, when executed, perform the method, the method comprising: receiving, at the computer system, from a first user device, a natural language utterance; performing, at the computer system, speech recognition to determine one or more words of the natural language utterance; receiving, at the computer system, from a second user device, a non-voice user input that relates to the natural language utterance, wherein the first user device and the second user device are associated with a user and are independently operable of one another and of the computer system; processing, at the computer system, the non-voice user input to determine context information for the one or more words; determining, at the computer system, one or more interpretations of the one or more words based on the context information; generating, at the computer system, a user request related to the natural language utterance based on the one or more interpretations; selecting, at the computer system, at least one user processing device to process the user request such that the selection of the user processing device is based on a determination that content related to the user request resides at the user processing device, wherein the user processing device comprises the first user device, the second user device, or a third user device independently operable of the computer system; and transmitting, by the computer system, the user request to the user processing device to invoke the user processing device to process the user request.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9229680B2;Method for facilitating access to data and functionality e.g. business-related software application, involves presenting user interface control to facilitate user access to software functionality with natural language input The method (150) involves receiving (152) natural language input. The natural language input is analyzed (156). The portions of the natural language input are selected. The portions are employed (158) to select software functionality. The user interface controls that are adapted to facilitate user access to the software functionality are presented (160) in combination with a representation of the natural language input. The keywords are automatically functionally augmented through in-line tagging of the keywords through the user interface controls. INDEPENDENT CLAIMS are included for the following:an apparatus for facilitating access to data and functionality; anda processor-readable storage device storing program for facilitating access to data and functionality. Method for facilitating access to data and functionality such as enterprise such as business, university, government and military-related software application and accompanying actions and data. The enterprise data and functionality is efficiently accessed by enabling accurate detection of keywords and phrases occurring in natural language input. The drawing shows a flowchart illustrating the process for facilitating access to data and functionality. 150Method for facilitating access to data and functionality152Step for receiving natural language input156Step for analyzing natural language input158Step for employing portions to select software functionality160Step for presenting user interface controls in combination with representation of natural language input An apparatus comprising: a digital processor coupled to a display and to a non-transitory processor-readable storage device, wherein the processor-readable storage device includes one or more instructions executable by the digital processor to perform acts comprising: receiving natural language input; analyzing the natural language input and selecting one or more portions of the natural language input; employing the one or more portions to select software functionality, wherein the one or more portions include one or more keywords occurring in the natural language input; and presenting one or more user interface controls in combination with a representation of the natural language input, wherein the one or more user interface controls are adapted to facilitate user access to the software functionality, and wherein presenting further includes functionally augmenting the one or more keywords via in-line tagging of the one or more keywords via one or more of the user interface controls, resulting in functionally-augmented natural language.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KR2017115501A;Server computer such as desktop computer, has computing device to generate labeled data set by pairing digital voice input with first user selection and updating selected language understanding classifier The server computer (140) has a computing device (102) communicatively coupled to the server computer. An intent associated with an action performs a function of a category of functions for a domain and a slot indicates a value for performing the action. A labeled data set is generated by pairing the digital voice input with a first user selection. A language understanding classifier associated with agent definitions is selected based on the intent. The selected language understanding classifier is updated based on the generated labeled data set. INDEPENDENT CLAIMS are included for the following:a method for updating language understanding classifier models; anda computer-readable storage medium storing instructions for updating language understanding classifier models. Server computer such as desktop computer, television screen, set-top box or gaming console used for mobile computing device in cloud computing environment. Uses include but are not limited to computing devices such as cell phone, smartphone, handheld computer, laptop computer, notebook computer, tablet device, netbook, media player, personal digital assistant (PDA) and video camera. A user selection is received using a graphical user interface of an end-user labeling tool (EULT) of the computing device. The digital personal assistant runs on the computing device and allows the user of the computing device to perform various actions using voice input. An automatic classifier update is prevented in instances when a user tries to associate a task with a domain, intent, and slot that most of the other remaining users in the system do not associate such utterance with. The drawing shows the block diagram architecture for updating language understanding classifier models. 102Computing device118Labeling tool120,170Language understanding classifier model140Server computer142Remote service A server computer, wherein: the processing unit the server computer is the server computer, and the memory of being connected to the processing unit are included, and the server computer is configured to perform the operation of updating the language understanding classification machine model, and the operation relates to at least one action which includes the operation of receiving the first user option about at least one among multiple computing devices which are communicable are connected to the server computer from at least one computing device among multiple intentions among at least one slot about at least one intention and/or at least one intention and in which the intention of (at least one is used to perform at least one function among the category of the function about the domain, and at least one slot shows the value which is used to perform at least one action, and the first user option relates to the digital speech input received by at least one computing device the operation receives the same multiple subsequent user options as the first user option among moreover, the multiple computing devices from at least one dissimilar computing device; and it further includes the operation of producing the labeled data set, the operation (the selection is performed based on at least one intention) choosing one language understanding classification machine from the multiple usable language understanding classification machines associated with at least one agent definition (agent definitions), and the operation of updating the selected language understanding classification machine based on the above-mentioned generated and labeled data set by making the digital speech input pair with the first user option.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN107210033A;Server computer such as desktop computer, has computing device to generate labeled data set by pairing digital voice input with first user selection and updating selected language understanding classifier The server computer (140) has a computing device (102) communicatively coupled to the server computer. An intent associated with an action performs a function of a category of functions for a domain and a slot indicates a value for performing the action. A labeled data set is generated by pairing the digital voice input with a first user selection. A language understanding classifier associated with agent definitions is selected based on the intent. The selected language understanding classifier is updated based on the generated labeled data set. INDEPENDENT CLAIMS are included for the following:a method for updating language understanding classifier models; anda computer-readable storage medium storing instructions for updating language understanding classifier models. Server computer such as desktop computer, television screen, set-top box or gaming console used for mobile computing device in cloud computing environment. Uses include but are not limited to computing devices such as cell phone, smartphone, handheld computer, laptop computer, notebook computer, tablet device, netbook, media player, personal digital assistant (PDA) and video camera. A user selection is received using a graphical user interface of an end-user labeling tool (EULT) of the computing device. The digital personal assistant runs on the computing device and allows the user of the computing device to perform various actions using voice input. An automatic classifier update is prevented in instances when a user tries to associate a task with a domain, intent, and slot that most of the other remaining users in the system do not associate such utterance with. The drawing shows the block diagram architecture for updating language understanding classifier models. 102Computing device118Labeling tool120,170Language understanding classifier model140Server computer142Remote service A server computer, comprising: a processing unit, and a memory coupled to the processing unit, the server computer is configured to execute operation for updating language understanding classifier model, the operations comprising: from the communicatively coupled to at least one computing device in the plurality of computing device receiving at least one of the server computer of the first user selection of the following items: a plurality of available at least one intent and/or for at least one slot of the at least one intended. wherein the at least one intent with the at least one action to perform at least one function type of the domain associated with the at least one groove indicating value for performing the at least one action; and the first user selection is associated with a position in the at least one computing device the received digital voice input, and from the plurality of computing device of at least one other computing device received with the first user selecting one of a plurality of subsequent users of the same selected through the digital voice input and the first user selection to pair to generate a mark data set; the marked data set from a plurality of available language associated with one or more proxy defined understanding of language understanding classifier selected from the classifier, the selecting is based at least on the at least one intention, and based on the generated to update the selected language understanding classifier.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160225370A1;Server computer such as desktop computer, has computing device to generate labeled data set by pairing digital voice input with first user selection and updating selected language understanding classifier The server computer (140) has a computing device (102) communicatively coupled to the server computer. An intent associated with an action performs a function of a category of functions for a domain and a slot indicates a value for performing the action. A labeled data set is generated by pairing the digital voice input with a first user selection. A language understanding classifier associated with agent definitions is selected based on the intent. The selected language understanding classifier is updated based on the generated labeled data set. INDEPENDENT CLAIMS are included for the following:a method for updating language understanding classifier models; anda computer-readable storage medium storing instructions for updating language understanding classifier models. Server computer such as desktop computer, television screen, set-top box or gaming console used for mobile computing device in cloud computing environment. Uses include but are not limited to computing devices such as cell phone, smartphone, handheld computer, laptop computer, notebook computer, tablet device, netbook, media player, personal digital assistant (PDA) and video camera. A user selection is received using a graphical user interface of an end-user labeling tool (EULT) of the computing device. The digital personal assistant runs on the computing device and allows the user of the computing device to perform various actions using voice input. An automatic classifier update is prevented in instances when a user tries to associate a task with a domain, intent, and slot that most of the other remaining users in the system do not associate such utterance with. The drawing shows the block diagram architecture for updating language understanding classifier models. 102Computing device118Labeling tool120,170Language understanding classifier model140Server computer142Remote service A server computer, comprising: a processing unit; and memory coupled to the processing unit; the server computer configured to perform operations for updating language understanding classifier models, the operations comprising: receiving from at least one computing device of a plurality of computing devices communicatively coupled to the server computer, a first user selection of at least one of the following: at least one intent of a plurality of available intents and/or at least one slot for the at least one intent, wherein: the at least one intent is associated with at least one action used to perform at least one function of a category of functions for a domain; the at least one slot indicating a value used for performing the at least one action; and the first user selection associated with a digital voice input received at the at least one computing device; and upon receiving from at least another computing device of the plurality of computing devices, a plurality of subsequent user selections that are identical to the first user selection: generating a labeled data set by pairing the digital voice input with the first user selection; selecting a language understanding classifier from a plurality of available language understanding classifiers associated with one or more agent definitions, the selecting based at least on the at least one intent; and updating the selected language understanding classifier based on the generated labeled data set.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SG11201705873A1;Server computer such as desktop computer, has computing device to generate labeled data set by pairing digital voice input with first user selection and updating selected language understanding classifier The server computer (140) has a computing device (102) communicatively coupled to the server computer. An intent associated with an action performs a function of a category of functions for a domain and a slot indicates a value for performing the action. A labeled data set is generated by pairing the digital voice input with a first user selection. A language understanding classifier associated with agent definitions is selected based on the intent. The selected language understanding classifier is updated based on the generated labeled data set. INDEPENDENT CLAIMS are included for the following:a method for updating language understanding classifier models; anda computer-readable storage medium storing instructions for updating language understanding classifier models. Server computer such as desktop computer, television screen, set-top box or gaming console used for mobile computing device in cloud computing environment. Uses include but are not limited to computing devices such as cell phone, smartphone, handheld computer, laptop computer, notebook computer, tablet device, netbook, media player, personal digital assistant (PDA) and video camera. A user selection is received using a graphical user interface of an end-user labeling tool (EULT) of the computing device. The digital personal assistant runs on the computing device and allows the user of the computing device to perform various actions using voice input. An automatic classifier update is prevented in instances when a user tries to associate a task with a domain, intent, and slot that most of the other remaining users in the system do not associate such utterance with. The drawing shows the block diagram architecture for updating language understanding classifier models. 102Computing device118Labeling tool120,170Language understanding classifier model140Server computer142Remote service ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MX2017009711A;Server computer such as desktop computer, has computing device to generate labeled data set by pairing digital voice input with first user selection and updating selected language understanding classifier The server computer (140) has a computing device (102) communicatively coupled to the server computer. An intent associated with an action performs a function of a category of functions for a domain and a slot indicates a value for performing the action. A labeled data set is generated by pairing the digital voice input with a first user selection. A language understanding classifier associated with agent definitions is selected based on the intent. The selected language understanding classifier is updated based on the generated labeled data set. INDEPENDENT CLAIMS are included for the following:a method for updating language understanding classifier models; anda computer-readable storage medium storing instructions for updating language understanding classifier models. Server computer such as desktop computer, television screen, set-top box or gaming console used for mobile computing device in cloud computing environment. Uses include but are not limited to computing devices such as cell phone, smartphone, handheld computer, laptop computer, notebook computer, tablet device, netbook, media player, personal digital assistant (PDA) and video camera. A user selection is received using a graphical user interface of an end-user labeling tool (EULT) of the computing device. The digital personal assistant runs on the computing device and allows the user of the computing device to perform various actions using voice input. An automatic classifier update is prevented in instances when a user tries to associate a task with a domain, intent, and slot that most of the other remaining users in the system do not associate such utterance with. The drawing shows the block diagram architecture for updating language understanding classifier models. 102Computing device118Labeling tool120,170Language understanding classifier model140Server computer142Remote service 1. One of server computer which comprising: one unit of processing; and coupled to the memory of processing unit; the computer of server configured for performing operations for update of classifier models of understanding of language, the operations comprising: receiving, of to the less one of computing device of one of plurality of devices communicatively coupled to the computer of computer server, one of first selection of user to the less one of the following; to the less one attempt of an plurality of attempts and/or available to the less one slot for to the less one attempt, where in: the to the less one attempt is associated with to the less one used for performing action to the less one function of one category of functions for one domain; the to the less one slot indicating one value used for performing to the less one action; and the first selection of user associated with an entrance of digital voice is received to the less one of computing device; and to the receiving of less to the other of computing device of the plurality of devices of computer, one plurality of selections of subsequent user which are identical to the first selection of user: generating one set of labeled data match to the entrance of the digital voice with the first selection of user; selecting one classifier of understanding language of one of plurality of classifiers of understanding of language available associated with one or more definitions of agent, the selection based To The less in one attempt; and updating the classifier of understanding of language selected based in the set of labeled data generated.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9508339B2;Server computer such as desktop computer, has computing device to generate labeled data set by pairing digital voice input with first user selection and updating selected language understanding classifier The server computer (140) has a computing device (102) communicatively coupled to the server computer. An intent associated with an action performs a function of a category of functions for a domain and a slot indicates a value for performing the action. A labeled data set is generated by pairing the digital voice input with a first user selection. A language understanding classifier associated with agent definitions is selected based on the intent. The selected language understanding classifier is updated based on the generated labeled data set. INDEPENDENT CLAIMS are included for the following:a method for updating language understanding classifier models; anda computer-readable storage medium storing instructions for updating language understanding classifier models. Server computer such as desktop computer, television screen, set-top box or gaming console used for mobile computing device in cloud computing environment. Uses include but are not limited to computing devices such as cell phone, smartphone, handheld computer, laptop computer, notebook computer, tablet device, netbook, media player, personal digital assistant (PDA) and video camera. A user selection is received using a graphical user interface of an end-user labeling tool (EULT) of the computing device. The digital personal assistant runs on the computing device and allows the user of the computing device to perform various actions using voice input. An automatic classifier update is prevented in instances when a user tries to associate a task with a domain, intent, and slot that most of the other remaining users in the system do not associate such utterance with. The drawing shows the block diagram architecture for updating language understanding classifier models. 102Computing device118Labeling tool120,170Language understanding classifier model140Server computer142Remote service A server computer, comprising: a processing unit; and memory coupled to the processing unit; the server computer configured to perform operations for updating language understanding classifier models, the operations comprising: receiving from at least one computing device of a plurality of computing devices communicatively coupled to the server computer, a first user selection of at least one of the following: at least one intent of a plurality of available intents and/or at least one slot for the at least one intent, wherein: the at least one intent is associated with at least one action used to perform at least one function of a category of functions for a domain; the at least one slot indicating a value used for performing the at least one action; and the first user selection associated with a digital voice input received at the at least one computing device; and upon receiving from at least another computing device of the plurality of computing devices, a plurality of subsequent user selections that are identical to the first user selection and a plurality of subsequent digital voice inputs corresponding to the plurality of subsequent user selections, wherein the plurality of subsequent digital voice inputs are substantially similar to the digital voice input: generating a labeled data set by pairing the digital voice input with the first user selection; selecting a language understanding classifier from a plurality of available language understanding classifiers associated with one or more agent definitions, the selecting based at least on the at least one intent; and updating the selected language understanding classifier based on the generated labeled data set.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20150088499A1;Method for facilitating access to data and functionality e.g. business-related software application, involves presenting user interface control to facilitate user access to software functionality with natural language input The method (150) involves receiving (152) natural language input. The natural language input is analyzed (156). The portions of the natural language input are selected. The portions are employed (158) to select software functionality. The user interface controls that are adapted to facilitate user access to the software functionality are presented (160) in combination with a representation of the natural language input. The keywords are automatically functionally augmented through in-line tagging of the keywords through the user interface controls. INDEPENDENT CLAIMS are included for the following:an apparatus for facilitating access to data and functionality; anda processor-readable storage device storing program for facilitating access to data and functionality. Method for facilitating access to data and functionality such as enterprise such as business, university, government and military-related software application and accompanying actions and data. The enterprise data and functionality is efficiently accessed by enabling accurate detection of keywords and phrases occurring in natural language input. The drawing shows a flowchart illustrating the process for facilitating access to data and functionality. 150Method for facilitating access to data and functionality152Step for receiving natural language input156Step for analyzing natural language input158Step for employing portions to select software functionality160Step for presenting user interface controls in combination with representation of natural language input A method for facilitating access to data and functionality, the method comprising: receiving natural language input; analyzing the natural language input and selecting one or more portions of the natural language input; employing the one or more portions to select software functionality; and presenting one or more user interface controls in combination with a representation of the natural language input, wherein the one or more user interface controls are adapted to facilitate user access to the software functionality.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JP05663031B2;Method for hybrid processing in natural language voice services environment, involves resolving multi-modal interaction at electronic device based on information contained in multiple messages received from virtual router The method involves detecting multi-modal interaction at an electronic device e.g. mobile phone. Multiple messages containing information related to the multi-modal interaction is communicated to a virtual router in communication with the electronic device. Multiple messages containing information related to intent of the multi-modal interaction is received by the electronic device. The multi-modal interaction at the electronic device is resolved based on the information contained in multiple messages received from the virtual router. INDEPENDENT CLAIMS are also included for the following:an electronic device for hybrid processing in a natural language voice services environmenta virtual router for hybrid processing in a natural language voice services environment. Method for hybrid processing in natural language voice services environment using an electronic device e.g. microphone, personal navigation device, mobile phone, Voice over Internet Protocol (VoIP) node, personal computer, media device, embedded device and server, and a virtual router (all claimed). The method enables performing hybrid processing in the natural language voice services environment using the electronic device and/or the virtual router without degrading performance in an efficient manner. The drawing shows a block diagram of a voice-enabled device for hybrid processing in a natural language voice services environment.100Voice-enabled device112Input devices120Automatic speech recognizer130Conversational language processor134Applications The process at which a 1st client apparatus transmits the information relevant to the 1st local time-measurement mechanism of a said 1st client apparatus to a virtual router,The process from which a said 1st client apparatus detects at least 1 natural language speech,The process at which a said 1st client apparatus transmits the information relevant to the said natural language speech to the said virtual router,The process of receiving the information in which a said 1st client apparatus specifies intent of the said natural language speech from the said virtual router,Said information that specifies intent of the said natural language speech here,(1) Said information relevant to said 1st local time-measurement mechanism transmitted by said 1st client apparatus,(2) Information relevant to 2nd local time-measurement mechanism of 2nd client apparatus,(3) Said information relevant to said natural language speech,and(4) It is based on information relevant to non voice input transmitted by said 2nd client apparatus,The process at which the said client apparatus solves the said natural language speech based on the said information that specifies intent of the said natural language speech,The hybrid processing method in a natural language voice service environment provided with these.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KR2017070094A;Method for receiving and processing request using voice and connection platform, involves performing natural language understanding based on speech recognition of audio input, and taking action based on context of user and understanding The method involves receiving audio input from a user (112a) requesting action at a client device (106a) responsive to an internal event. Automatic speech recognition is performed on the audio input. Context of user is obtained. Natural language understanding is performed based on the speech recognition of the audio input. The action is taken based on the context of the user and the natural language understanding. Voice assistant is initiated without user input before receiving the audio input from the user. The context includes context history, dialogue history, a user profile, user history, a location and a current context domain. An INDEPENDENT CLAIM is also included for a system for receiving and processing a request using a voice and connection platform. Method for receiving and processing a request using a voice and connection platform. The method enables utilizing agnostic and modular approach to automatic speech recognition, natural language understanding and text to speech components, thus allowing frequent updates to components, and simplifying adaptation of a system to different languages. The method enables managing context so as to provide natural and humanlike dialogue with the user and increase accuracy of understanding of the user's requests, while reducing amount of time between receiving the request and execution of the request. The method enables increasing system's ability to accurately recognize words from speech, determine user's intended request and facilitate more natural dialog between the user and system. The drawing shows a schematic block diagram of a system for facilitating a voice and connection platform. 106a-106nClient devices110Automatic speech recognition server112a-112nUsers116Text to speech server122Voice and connection server A method, wherein: The method comprises the step of receiving the first audio input as the method from the user requesting the first action in the first device; the step of performing the automatic speech recognition about the first audio input; the step of obtaining the context of the user; the step of performing the natural language understanding based on the voice recognition of the first audio input; and the step of taking the first action based on the context and natural language understanding of the user.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN107004410A;Method for receiving and processing request using voice and connection platform, involves performing natural language understanding based on speech recognition of audio input, and taking action based on context of user and understanding The method involves receiving audio input from a user (112a) requesting action at a client device (106a) responsive to an internal event. Automatic speech recognition is performed on the audio input. Context of user is obtained. Natural language understanding is performed based on the speech recognition of the audio input. The action is taken based on the context of the user and the natural language understanding. Voice assistant is initiated without user input before receiving the audio input from the user. The context includes context history, dialogue history, a user profile, user history, a location and a current context domain. An INDEPENDENT CLAIM is also included for a system for receiving and processing a request using a voice and connection platform. Method for receiving and processing a request using a voice and connection platform. The method enables utilizing agnostic and modular approach to automatic speech recognition, natural language understanding and text to speech components, thus allowing frequent updates to components, and simplifying adaptation of a system to different languages. The method enables managing context so as to provide natural and humanlike dialogue with the user and increase accuracy of understanding of the user's requests, while reducing amount of time between receiving the request and execution of the request. The method enables increasing system's ability to accurately recognize words from speech, determine user's intended request and facilitate more natural dialog between the user and system. The drawing shows a schematic block diagram of a system for facilitating a voice and connection platform. 106a-106nClient devices110Automatic speech recognition server112a-112nUsers116Text to speech server122Voice and connection server A method, comprising: receiving a first audio input first action request from the user at the first device, the first audio input executing automatic voice identification, obtaining the context of the user based on the first audio input by the voice recognition to perform natural language understanding and natural language understanding based on the context and the said user to take said first action.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN107507614A;Method for executing natural language command of user at user interface state in device, involves sending natural language command to natural language processing device when natural language command satisfies predetermined condition The method involves a detecting whether a user interface (UI) state is changed. Corresponding related information is obtained if the UI state is changed. The related information is sent to a language processing device corresponding to user voice interactive command. Natural language command input of a user is obtained. The natural language command is sent to the natural language processing device when the natural language command satisfies predetermined condition. Voice interaction process is performed in an intelligent device when the UI state is changed. An INDEPENDENT CLAIM is also included for a computer readable storage medium comprising a set of instructions for executing natural language command of a user at a combined UI state in an intelligent device. Method for executing natural language command of a user at a combined UI state in an intelligent device e.g. vehicle intelligent voice device, intelligent sound TV, intelligent sound box and computing device (all claimed). The method enables realizing user voice interaction process at the UI state to execute the natural language command of the user so as to improve interactive experience of the user. The drawing shows a flow diagram illustrating a method for executing natural language command of a user at a combined UI state in an intelligent device. '(Drawing includes non-English language text)' A method of intelligent device for combined UI state executing the natural language command of user, wherein the method comprises the following steps: a detecting UI state is changed, b if changes, obtaining the corresponding related information; c, sending the related information of the used to respond to user voice interactive command to the corresponding natural language processing device, a natural language command d, wherein obtaining user input, the natural language command satisfies the predetermined condition, e the natural language command to the natural language processing device.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160098992A1;Method for receiving and processing request using voice and connection platform, involves performing natural language understanding based on speech recognition of audio input, and taking action based on context of user and understanding The method involves receiving audio input from a user (112a) requesting action at a client device (106a) responsive to an internal event. Automatic speech recognition is performed on the audio input. Context of user is obtained. Natural language understanding is performed based on the speech recognition of the audio input. The action is taken based on the context of the user and the natural language understanding. Voice assistant is initiated without user input before receiving the audio input from the user. The context includes context history, dialogue history, a user profile, user history, a location and a current context domain. An INDEPENDENT CLAIM is also included for a system for receiving and processing a request using a voice and connection platform. Method for receiving and processing a request using a voice and connection platform. The method enables utilizing agnostic and modular approach to automatic speech recognition, natural language understanding and text to speech components, thus allowing frequent updates to components, and simplifying adaptation of a system to different languages. The method enables managing context so as to provide natural and humanlike dialogue with the user and increase accuracy of understanding of the user's requests, while reducing amount of time between receiving the request and execution of the request. The method enables increasing system's ability to accurately recognize words from speech, determine user's intended request and facilitate more natural dialog between the user and system. The drawing shows a schematic block diagram of a system for facilitating a voice and connection platform. 106a-106nClient devices110Automatic speech recognition server112a-112nUsers116Text to speech server122Voice and connection server A method comprising: receiving, at a first device, a first audio input from a user requesting a first action; performing automatic speech recognition on the first audio input; obtaining a context of user; performing natural language understanding based on the speech recognition of the first audio input; and taking the first action based on the context of the user and the natural language understanding.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9437206B2;Method for enabling and enhancing use of voice control in voice controlled application through development framework, involves permitting association formed between action-context pair and application handler in voice controlled application The method involves providing a set of framework action-context pairs used in a memory of an application development device i.e. user equipment, where the framework context defines a list of parameters related to an action and respective value types. The action-context pair is matched with semantically related vocabulary by a voice recognition engine (VRE) (152) and a natural language library (154). An association is permitted to be formed between an action-context pair and application handlers (112) in the voice controlled application. INDEPENDENT CLAIMS are also included for the following:a non-transitory computer program product comprising a set of instructions for enabling and enhancing use of voice control in a voice controlled applicationa system for developing a voice controlled applicationan user equipment comprising a voice controlled application executable file using action-context pairs and associated VCA application handlers. Method for enabling and enhancing use of voice control in a voice controlled application (VCA) through a development framework. The method enables permitting an application developer to easily and quickly create voice controlled applications that can be run from a user equipment (UE), and easily integrate new voice commands and functionality into his application. The drawing shows a block diagram for a system for enabling and enhancing use of voice control in a voice controlled application. 112Application handlers150Backend engine152VRE154Natural language library250Registration table A method for enabling or enhancing a use of voice control in a voice controlled application (VCA) via a development framework, the method comprising: providing in the development framework a plurality of framework action-context pairs wherein the development framework is stored in a memory of an application development device comprising a processor, the plurality of framework action-context pairs being configured to direct execution of the VCA, wherein each framework action-context pair has an action and a context that defines a list of parameters related to the action and respective value types of the parameters; providing at least one of a voice recognition engine (VRE) or a natural language library to match each action-context pair with semantically related vocabulary; generating automatically a grammar to capture knowledge about natural language understanding in the context of said VCA; providing in the development framework a registration mechanism that forms an association between an action-context pair and a handler in the VCA, the registration mechanism such that when a voice input is received, the voice input is sent to a speech recognition engine based on said grammar to obtain an action-context pair from the plurality of framework action-context pairs and the action-context pair is used to select and trigger the handler in the VCA to execute the action intended by the voice input.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9601114B2;Method for natural language processing of speech utterance input to computerized system, involves automatically transmitting audio message based on message portion of speech utterance using channel and destinations indicated by parameters The method (100) involves parsing a natural language speech utterance to identify a communication portion of an utterance and a start and an end of a message portion within the speech utterance. The communication portion is analyzed to determine communication parameters, where the communication parameter indicates a destination or channel for message delivery. An audio message is automatically transmitted based on the message portion of the speech utterance using the channel and destinations indicated by the communication parameters. The destination is a voice mail mailbox, an electronic mail address and a user account on a social networking website. An INDEPENDENT CLAIM is also included for a system for processing a natural language message in a computerized system. Method for natural language processing of a speech utterance input to a computerized system. Uses include but are not limited to a smartphone, a tablet computer, a telephone, a laptop computer, a cellular telephone, a desktop computer and a workstation. The method enables assigning a probability weight to each parse to allow disambiguation and choice of the most likely parses based on all factors available, thus improving reliability and flexibility of the computerized system. The method enables anticipating that a larger number of telephone system providers facilitates incoming communications by setting out publicly accessible interface ports as an automated communication becomes more common, so that a user can quickly add nicknames to already-identified contact listings and vast majority of intended recipients can be identified using the nickname as the user employs in ordinary conversation. The drawing shows a flow chart illustrating a method for combining a communication portion with a message in a natural language computer system. 100Method for natural language processing of speech utterance input102Step for receiving natural language communication104Step for identifying combined voice mail/ command106Step for parsing content110Step for saving message A method for natural language processing of a speech utterance input to a computerized system, comprising: receiving and storing a speech utterance in a computerized system, the speech utterance comprising words spoken by a user, the spoken words including a communication portion and a message portion; parsing the natural language speech utterance to identify the communication portion of the utterance and the start and the end of the message portion within the speech utterance; analyzing the communication portion to determine one or more communication parameters, a communication parameter indicating a destination or channel for message delivery; and automatically transmitting an audio message based on the message portion of the speech utterance using a channel and one or more destinations indicated by the one or more communication parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160171981A1;Method for natural language processing of speech utterance input to computerized system, involves automatically transmitting audio message based on message portion of speech utterance using channel and destinations indicated by parameters The method (100) involves parsing a natural language speech utterance to identify a communication portion of an utterance and a start and an end of a message portion within the speech utterance. The communication portion is analyzed to determine communication parameters, where the communication parameter indicates a destination or channel for message delivery. An audio message is automatically transmitted based on the message portion of the speech utterance using the channel and destinations indicated by the communication parameters. The destination is a voice mail mailbox, an electronic mail address and a user account on a social networking website. An INDEPENDENT CLAIM is also included for a system for processing a natural language message in a computerized system. Method for natural language processing of a speech utterance input to a computerized system. Uses include but are not limited to a smartphone, a tablet computer, a telephone, a laptop computer, a cellular telephone, a desktop computer and a workstation. The method enables assigning a probability weight to each parse to allow disambiguation and choice of the most likely parses based on all factors available, thus improving reliability and flexibility of the computerized system. The method enables anticipating that a larger number of telephone system providers facilitates incoming communications by setting out publicly accessible interface ports as an automated communication becomes more common, so that a user can quickly add nicknames to already-identified contact listings and vast majority of intended recipients can be identified using the nickname as the user employs in ordinary conversation. The drawing shows a flow chart illustrating a method for combining a communication portion with a message in a natural language computer system. 100Method for natural language processing of speech utterance input102Step for receiving natural language communication104Step for identifying combined voice mail/ command106Step for parsing content110Step for saving message A method for natural language processing of a speech utterance input to a computerized system, comprising: receiving and storing a speech utterance in a computerized system, the utterance comprising a communication portion and a message portion; parsing the natural language speech utterance to identify the communication portion of the utterance and the start and the end of the message portion within the speech utterance; analyzing the communication portion to determine one or more communication parameters, a communication parameter indicating a destination or channel for message delivery; and automatically transmitting an audio message based on the message portion of the speech utterance using a channel and one or more destinations indicated by the one or more communication parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9576576B2;Computer-implemented method for identifying content item data based on environmental audio data and spoken natural language query, involves providing answer to question that is asked about item of media content The method involves generating an audio recording of a spoken, natural language question that has been asked about an item of media content and that does not name, or request the name of, the item of media content. The audio recording of ambient sounds that are associated with the playback of the item of media content and that are recorded contemporaneously with the asked question is generated. An answer is provided to the question that has been asked about the item of media content, in response to the question. A media content is identified (210) based on the environmental image data. INDEPENDENT CLAIMS are included for the following:a system for identifying content item data based on environmental audio data and spoken natural language query; anda non-transitory computer-readable medium storing a program for identifying content item data based on environmental audio data and spoken natural language query. Computer-implemented method for identifying content item data based on environmental audio data and spoken natural language query. By selecting the first index, the content recognition engine more efficiently identify the content item data. The drawing shows a flowchart for the process for identifying content item data based on environmental audio data and spoken natural language query. 202Step for receiving the audio data and environmental audio data204Step for obtaining the transcription of natural language query206Step for determining the particular content type208Step for providing the environmental audio data210Step for identifying the media content based on the environmental image data A computer-implemented method comprising: generating, by a mobile device, an audio recording of (i) a spoken, natural language question that has been asked about an item of media content and that does not name, or request the name of, the item of media content, and (ii) ambient sounds that are associated with the playback of the item of media content and that are recorded contemporaneously with the question being asked; forwarding the audio recording to a coordination engine server; receiving from the coordination engine server an answer to the question, the answer based on processing of the question by a query processing engine server and on processing of the ambient sounds by a content identification engine server; and in response to the question, providing, by the mobile device, an answer to the question that has been asked about the item of media content.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KR2017103801A;Computing device for headlessly completing a task of an application in the background of a digital personal assistant, comprises a processing unit, where computing device is configured with a speech-controlled digital personal assistant The computing device (130) comprises a processing unit, a memory, and microphones (150). The computing device is configured with a speech-controlled digital personal assistant (120), where the operations comprises receiving speech input generated by a user via the microphones. The speech recognition is performed using the speech input to determine a spoken command, where the spoken command comprises a request to perform a task of a third-party application (110). The task (112) is identified using a data structure (140) that defines tasks of third-party applications invokable by spoken command. INDEPENDENT CLAIMS are included for the following:a method for headlessly completing a task of an application in the background of a digital personal assistant; anda computer-readable storage medium for storing computer-executable instructions for causing a computing device to perform operations for completing a task of a voice-enabled application. Computing device for headlessly completing a task of an application in the background of a digital personal assistant. The voice-enabled application executes the task as a background task and within a context of the voice-controlled digital personal assistant without a user interface of the voice-enabled application surfacing. The responses provided to the user from the voice-controlled digital personal assistant are selected from display text, text-to-speech, deep-link user resource identifier, Hypertext markup language, list template, physical address, or telephone number. The drawing shows a schematic block diagram of a system for headlessly completing a task of an application. 110Third-party application112Task120Speech-controlled digital personal assistant130Computing device140Data structure150Microphones A computing device, wherein: the computing device comprises the processing unit as the computing device, the memory, and at least one microphone, and the computing device is configured to perform operations with the voice - control digital private secretary, and operations include the operation of receiving the generated voice input, and the request through at least one microphone by the user, and as to the request, the operation - voice command performing the voice recognition using the voice input and determines the voice command performs the operation of the third party application, and the operation comprises the operation which is determined that the operation is discriminated using the data construct of defining the operation of the third party application which can call with the voice command of determining the paper in which the operation of the third party application is performed to the headless mode, and the operation of the third party application can be performed to the headless mode; it provides the response for the user with the operation, of the third party application being performed as the background process and executing the operation to the headless mode the operation of receiving the response showing state associated with the operation from the third party application, and the user interface of the voice - control digital private secretary based on the above-mentioned received state associated with the operation; and the response does not show the user interface of the third party application and is generated in the context of the user interface of the voice - control digital private secretary.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN107111516A;Computing device for headlessly completing a task of an application in the background of a digital personal assistant, comprises a processing unit, where computing device is configured with a speech-controlled digital personal assistant The computing device (130) comprises a processing unit, a memory, and microphones (150). The computing device is configured with a speech-controlled digital personal assistant (120), where the operations comprises receiving speech input generated by a user via the microphones. The speech recognition is performed using the speech input to determine a spoken command, where the spoken command comprises a request to perform a task of a third-party application (110). The task (112) is identified using a data structure (140) that defines tasks of third-party applications invokable by spoken command. INDEPENDENT CLAIMS are included for the following:a method for headlessly completing a task of an application in the background of a digital personal assistant; anda computer-readable storage medium for storing computer-executable instructions for causing a computing device to perform operations for completing a task of a voice-enabled application. Computing device for headlessly completing a task of an application in the background of a digital personal assistant. The voice-enabled application executes the task as a background task and within a context of the voice-controlled digital personal assistant without a user interface of the voice-enabled application surfacing. The responses provided to the user from the voice-controlled digital personal assistant are selected from display text, text-to-speech, deep-link user resource identifier, Hypertext markup language, list template, physical address, or telephone number. The drawing shows a schematic block diagram of a system for headlessly completing a task of an application. 110Third-party application112Task120Speech-controlled digital personal assistant130Computing device140Data structure150Microphones A computing device, comprising: a processing unit, a memory, and one or more microphone; said computing device configured with voice control of a digital personal assistant, each operation comprising: receiving speech input generated by a user via said one or more microphone; performing voice recognition of the voice input is used to determine the spoken command, wherein the command comprises executing the third party application request for task, and wherein the task is a third-party application using defined by spoken command invoking a task data structure for identification; determining whether the third party application task can be headless executing, when it is determined that the third party application task can be Headless executed, cause the third party application executing as a background process to Headless performing the task; receiving information associated with the task instruction from the third party application the state of response, and through the voice control of a digital personal assistant of the user interface to provide a response to the user based on the state associated with the task received. in such that the response from the voice control of a digital personal assistant of the user interface context, and there is no user interface used by the third party.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160343371A1;Computer-implemented method for identifying content item data based on environmental audio data and spoken natural language query, involves providing answer to question that is asked about item of media content The method involves generating an audio recording of a spoken, natural language question that has been asked about an item of media content and that does not name, or request the name of, the item of media content. The audio recording of ambient sounds that are associated with the playback of the item of media content and that are recorded contemporaneously with the asked question is generated. An answer is provided to the question that has been asked about the item of media content, in response to the question. A media content is identified (210) based on the environmental image data. INDEPENDENT CLAIMS are included for the following:a system for identifying content item data based on environmental audio data and spoken natural language query; anda non-transitory computer-readable medium storing a program for identifying content item data based on environmental audio data and spoken natural language query. Computer-implemented method for identifying content item data based on environmental audio data and spoken natural language query. By selecting the first index, the content recognition engine more efficiently identify the content item data. The drawing shows a flowchart for the process for identifying content item data based on environmental audio data and spoken natural language query. 202Step for receiving the audio data and environmental audio data204Step for obtaining the transcription of natural language query206Step for determining the particular content type208Step for providing the environmental audio data210Step for identifying the media content based on the environmental image data (canceled),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160203002A1;Computing device for headlessly completing a task of an application in the background of a digital personal assistant, comprises a processing unit, where computing device is configured with a speech-controlled digital personal assistant The computing device (130) comprises a processing unit, a memory, and microphones (150). The computing device is configured with a speech-controlled digital personal assistant (120), where the operations comprises receiving speech input generated by a user via the microphones. The speech recognition is performed using the speech input to determine a spoken command, where the spoken command comprises a request to perform a task of a third-party application (110). The task (112) is identified using a data structure (140) that defines tasks of third-party applications invokable by spoken command. INDEPENDENT CLAIMS are included for the following:a method for headlessly completing a task of an application in the background of a digital personal assistant; anda computer-readable storage medium for storing computer-executable instructions for causing a computing device to perform operations for completing a task of a voice-enabled application. Computing device for headlessly completing a task of an application in the background of a digital personal assistant. The voice-enabled application executes the task as a background task and within a context of the voice-controlled digital personal assistant without a user interface of the voice-enabled application surfacing. The responses provided to the user from the voice-controlled digital personal assistant are selected from display text, text-to-speech, deep-link user resource identifier, Hypertext markup language, list template, physical address, or telephone number. The drawing shows a schematic block diagram of a system for headlessly completing a task of an application. 110Third-party application112Task120Speech-controlled digital personal assistant130Computing device140Data structure150Microphones A computing device comprising: a processing unit; memory; and one or more microphones; the computing device configured with a speech-controlled digital personal assistant, the operations comprising: receiving speech input generated by a user via the one or more microphones; performing speech recognition using the speech input to determine a spoken command, wherein the spoken command comprises a request to perform a task of a third-party application, and wherein the task is identified using a data structure that defines tasks of third-party applications invokable by spoken command; determining whether the task of the third-party application is capable of being headlessly executed; causing the third-party application to execute as a background process to headlessly execute the task when it is determined that the task of the third-party application is capable of being headlessly executed; receiving a response from the third-party application indicating a state associated with the task; and providing, by a user interface of the speech-controlled digital personal assistant, a response to the user based on the received state associated with the task so that the response comes from within a context of user interface of the speech-controlled digital personal assistant without surfacing the user interface of the third-party application.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JP2017535823A;Method for receiving and processing request using voice and connection platform, involves performing natural language understanding based on speech recognition of audio input, and taking action based on context of user and understanding The method involves receiving audio input from a user (112a) requesting action at a client device (106a) responsive to an internal event. Automatic speech recognition is performed on the audio input. Context of user is obtained. Natural language understanding is performed based on the speech recognition of the audio input. The action is taken based on the context of the user and the natural language understanding. Voice assistant is initiated without user input before receiving the audio input from the user. The context includes context history, dialogue history, a user profile, user history, a location and a current context domain. An INDEPENDENT CLAIM is also included for a system for receiving and processing a request using a voice and connection platform. Method for receiving and processing a request using a voice and connection platform. The method enables utilizing agnostic and modular approach to automatic speech recognition, natural language understanding and text to speech components, thus allowing frequent updates to components, and simplifying adaptation of a system to different languages. The method enables managing context so as to provide natural and humanlike dialogue with the user and increase accuracy of understanding of the user's requests, while reducing amount of time between receiving the request and execution of the request. The method enables increasing system's ability to accurately recognize words from speech, determine user's intended request and facilitate more natural dialog between the user and system. The drawing shows a schematic block diagram of a system for facilitating a voice and connection platform. 106a-106nClient devices110Automatic speech recognition server112a-112nUsers116Text to speech server122Voice and connection server ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SG11201705282A1;Computing device for headlessly completing a task of an application in the background of a digital personal assistant, comprises a processing unit, where computing device is configured with a speech-controlled digital personal assistant The computing device (130) comprises a processing unit, a memory, and microphones (150). The computing device is configured with a speech-controlled digital personal assistant (120), where the operations comprises receiving speech input generated by a user via the microphones. The speech recognition is performed using the speech input to determine a spoken command, where the spoken command comprises a request to perform a task of a third-party application (110). The task (112) is identified using a data structure (140) that defines tasks of third-party applications invokable by spoken command. INDEPENDENT CLAIMS are included for the following:a method for headlessly completing a task of an application in the background of a digital personal assistant; anda computer-readable storage medium for storing computer-executable instructions for causing a computing device to perform operations for completing a task of a voice-enabled application. Computing device for headlessly completing a task of an application in the background of a digital personal assistant. The voice-enabled application executes the task as a background task and within a context of the voice-controlled digital personal assistant without a user interface of the voice-enabled application surfacing. The responses provided to the user from the voice-controlled digital personal assistant are selected from display text, text-to-speech, deep-link user resource identifier, Hypertext markup language, list template, physical address, or telephone number. The drawing shows a schematic block diagram of a system for headlessly completing a task of an application. 110Third-party application112Task120Speech-controlled digital personal assistant130Computing device140Data structure150Microphones ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MX2017008926A;Computing device for headlessly completing a task of an application in the background of a digital personal assistant, comprises a processing unit, where computing device is configured with a speech-controlled digital personal assistant The computing device (130) comprises a processing unit, a memory, and microphones (150). The computing device is configured with a speech-controlled digital personal assistant (120), where the operations comprises receiving speech input generated by a user via the microphones. The speech recognition is performed using the speech input to determine a spoken command, where the spoken command comprises a request to perform a task of a third-party application (110). The task (112) is identified using a data structure (140) that defines tasks of third-party applications invokable by spoken command. INDEPENDENT CLAIMS are included for the following:a method for headlessly completing a task of an application in the background of a digital personal assistant; anda computer-readable storage medium for storing computer-executable instructions for causing a computing device to perform operations for completing a task of a voice-enabled application. Computing device for headlessly completing a task of an application in the background of a digital personal assistant. The voice-enabled application executes the task as a background task and within a context of the voice-controlled digital personal assistant without a user interface of the voice-enabled application surfacing. The responses provided to the user from the voice-controlled digital personal assistant are selected from display text, text-to-speech, deep-link user resource identifier, Hypertext markup language, list template, physical address, or telephone number. The drawing shows a schematic block diagram of a system for headlessly completing a task of an application. 110Third-party application112Task120Speech-controlled digital personal assistant130Computing device140Data structure150Microphones 1. One of computing device that comprising: one unit of processing; memory; and one or more microphones; the device of computing system configured with one controlled personal digital assistant by voice, the operations comprising: receiving the entrance of generated by one voice to user by means of one or more of the microphones; performing recognition of speech using the entrance of voice for determining one of voice command, where the command is voice of comprising an application for performing one task of an application of third part, and is where the task are identified using one of structure that defines task of data of third party applications that may are invoked by the command of voice; determining if the task of application of the third part is capable of being executed without head; do what the application of third part are run as one process of fund or of second plane for run, without head, the task are determined when the task of that application of the third part is capable of being executed without head; receiving an response of the application of third part indicating one state associated with the task; and providing, by an interface of the personal digital assistant user controlled by voice, an answer to the user based in the received status associated with the task, so that the response of coming from inside of one of the context of user interface of personal digital assistant controlled by activating the voice of user interface of application of the third part.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PH12017550013A1;Server computer such as desktop computer, has computing device to generate labeled data set by pairing digital voice input with first user selection and updating selected language understanding classifier The server computer (140) has a computing device (102) communicatively coupled to the server computer. An intent associated with an action performs a function of a category of functions for a domain and a slot indicates a value for performing the action. A labeled data set is generated by pairing the digital voice input with a first user selection. A language understanding classifier associated with agent definitions is selected based on the intent. The selected language understanding classifier is updated based on the generated labeled data set. INDEPENDENT CLAIMS are included for the following:a method for updating language understanding classifier models; anda computer-readable storage medium storing instructions for updating language understanding classifier models. Server computer such as desktop computer, television screen, set-top box or gaming console used for mobile computing device in cloud computing environment. Uses include but are not limited to computing devices such as cell phone, smartphone, handheld computer, laptop computer, notebook computer, tablet device, netbook, media player, personal digital assistant (PDA) and video camera. A user selection is received using a graphical user interface of an end-user labeling tool (EULT) of the computing device. The digital personal assistant runs on the computing device and allows the user of the computing device to perform various actions using voice input. An automatic classifier update is prevented in instances when a user tries to associate a task with a domain, intent, and slot that most of the other remaining users in the system do not associate such utterance with. The drawing shows the block diagram architecture for updating language understanding classifier models. 102Computing device118Labeling tool120,170Language understanding classifier model140Server computer142Remote service ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9548066B2;Speech-based system, has intent router to provide indication of user intent to speech interface device that is responsive to user intent to perform action corresponding to user intent The system (100) has server applications (136) in conjunction with a speech interface device (102) located in premises of a user (106) to provide services. A speech processing component (130) receives utterances expressing user intents from device. The component performs automatic speech recognition and natural language understanding on utterances to determine intents. An intent router (138) invokes identified application to perform an action according to intent. An indication of second user intent is provided to device to perform second action according to second user intent. An INDEPENDENT CLAIM is included for a method for selecting and invoking user-installed applications in a speech-based or language-based system. Speech-based system for use with computing devices such as personal computers, media players, mobile devices e.g. smartphones, tablet computers. The control service provides functionality for allowing the user to identify and install applications that have been made available by various developers and vendors for execution on the speech interface device. The control service also provides a dialog management component configured to coordinate speech dialogs or interactions with the user in conjunction with the speech services. The speech interface device and the control service are communicatively coupled to the network through wired technologies e.g. universal serial bus (USB) , wireless technologies e.g. Bluetooth  or other connection technologies. The drawing shows the block diagram of the speech-based system having a control service and a speech interface device. 100Speech-based system102Speech interface device106User130Speech processing component136Server application138Intent router A system comprising: one or more server computers; one or more server applications that have been selected by a user for execution on the one or more server computers, wherein the one or more server applications operate in conjunction with a speech interface device located in premises of the user to provide services for the user; a speech processing component configured to receive, from the speech interface device, an audio signal that represents user speech, wherein the user speech expresses a user intent, the speech processing component being further configured to perform automatic speech recognition on the audio signal to identify the user speech and to perform natural language understanding on the user speech to determine the user intent; and an intent router configured to perform acts comprising: identifying a first server application of the one or more server applications corresponding to the user intent; providing a first indication to the first server application to invoke an action corresponding to the user intent; providing a second indication of the user intent to the speech interface device, wherein the speech interface device is responsive to the user intent to perform the action corresponding to the user intent; receiving, at the one or more server computers, a confirmation from the speech interface device that at least one of (i) the speech interface device will perform the action in response to the user intent or (ii) the speech interface device has performed the action in response to the user intent; and providing a third indication, based at least in part on receiving the confirmation, to the first server application to cancel responding to the user intent.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160042748A1;Speech-based system, has intent router to provide indication of user intent to speech interface device that is responsive to user intent to perform action corresponding to user intent The system (100) has server applications (136) in conjunction with a speech interface device (102) located in premises of a user (106) to provide services. A speech processing component (130) receives utterances expressing user intents from device. The component performs automatic speech recognition and natural language understanding on utterances to determine intents. An intent router (138) invokes identified application to perform an action according to intent. An indication of second user intent is provided to device to perform second action according to second user intent. An INDEPENDENT CLAIM is included for a method for selecting and invoking user-installed applications in a speech-based or language-based system. Speech-based system for use with computing devices such as personal computers, media players, mobile devices e.g. smartphones, tablet computers. The control service provides functionality for allowing the user to identify and install applications that have been made available by various developers and vendors for execution on the speech interface device. The control service also provides a dialog management component configured to coordinate speech dialogs or interactions with the user in conjunction with the speech services. The speech interface device and the control service are communicatively coupled to the network through wired technologies e.g. universal serial bus (USB) , wireless technologies e.g. Bluetooth  or other connection technologies. The drawing shows the block diagram of the speech-based system having a control service and a speech interface device. 100Speech-based system102Speech interface device106User130Speech processing component136Server application138Intent router A system comprising: one or more server computers; one or more server applications that have been selected and enabled by a user for execution on the one or more server computers, wherein the one or more selected and enabled server applications operate in conjunction with a speech interface device located in premises of the user to provide services for the user; a speech processing component configured to receive first and second utterances from the speech interface device, wherein the first and second utterances express first and second user intents, respectively, the speech processing component being further configured to perform automatic speech recognition and natural language understanding on the first and second utterances to determine the first and second user intents; an intent router configured to perform acts comprising: identifying a server application of the one or more server applications corresponding to the first user intent; invoking the identified server application to perform a first action corresponding to the first user intent; and providing an indication of the second user intent to the speech interface device, wherein the speech interface device is responsive to the second user intent to perform a second action corresponding to the second user intent.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN107004411A;Speech-based system, has intent router to provide indication of user intent to speech interface device that is responsive to user intent to perform action corresponding to user intent The system (100) has server applications (136) in conjunction with a speech interface device (102) located in premises of a user (106) to provide services. A speech processing component (130) receives utterances expressing user intents from device. The component performs automatic speech recognition and natural language understanding on utterances to determine intents. An intent router (138) invokes identified application to perform an action according to intent. An indication of second user intent is provided to device to perform second action according to second user intent. An INDEPENDENT CLAIM is included for a method for selecting and invoking user-installed applications in a speech-based or language-based system. Speech-based system for use with computing devices such as personal computers, media players, mobile devices e.g. smartphones, tablet computers. The control service provides functionality for allowing the user to identify and install applications that have been made available by various developers and vendors for execution on the speech interface device. The control service also provides a dialog management component configured to coordinate speech dialogs or interactions with the user in conjunction with the speech services. The speech interface device and the control service are communicatively coupled to the network through wired technologies e.g. universal serial bus (USB) , wireless technologies e.g. Bluetooth  or other connection technologies. The drawing shows the block diagram of the speech-based system having a control service and a speech interface device. 100Speech-based system102Speech interface device106User130Speech processing component136Server application138Intent router A system, comprising: one or a plurality of server computers, one or more server applications, which has been selected by the user and enabled for on the one or more server computers executing. wherein said one or more is selected and enabled server application is combined with the user position in the voice interface device to operate, providing service for the user, voice processing part, which is configured for receiving first and second speech from the voice interface device. wherein the first and second verbal expression first and second user intention, the voice processing part, in addition to the first and second utterance performs automatic speech recognition and natural language understanding to determine user intent by the first and second configuration, intended router. which is configured to perform operations comprising one of the following actions: identifying the one or more server applications in server application corresponding to said first user intention, transferring the identified server application to perform a first action corresponding to the first user intention; and provides an indication of the second user intention to the voice interface device, wherein said voice interface device in response to the second user intent to perform a second operation corresponding to the second user intent.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170161720A1;Wearable personal digital device for facilitating mobile device payments, has processor providing payment confirmation to user, which is provided using haptic feedback, and payment transaction is completed by near field communication The device (200) has a processor placed to detect a match of scanned user fingerprints with reference fingerprints, determined heart rate with a reference heart rate, and determined ECG with reference ECG, where the processor provides a code through a display of the device to a merchant digital device for performing a payment transaction. The processor provides a payment confirmation to a user (150), where the payment confirmation is provided using a haptic feedback, and the payment transaction is completed by near field communication. Wearable personal digital device i.e. wearable mobile devices such as wristwatch digital device, for facilitating mobile device payments and personal use. The device enables utilizing payment data, stored content data and access rules data to reduce a risk of an unauthorized access to a content data. The drawing shows a schematic view of an environment in which a wearable personal digital device for facilitating mobile device payments and personal use. 110Network120Mobile base station140External device150User200Wearable personal digital device A wearable personal digital device for facilitating mobile device payments, personal use, and health care the device comprising: a processor being operable to: receive data from an external device; based on the data, provide a notification to a user; receive a user input; and perform a command, the command being selected based on the user input; provide a natural language user interface to communicate with the user, the natural language user interface being operable to sense a user voice and provide a response in a natural language to the user; a near field communication (NFC) unit communicatively coupled to the processor; a display communicatively coupled to the processor, the display including a touchscreen, wherein the display includes a force sensor, wherein the force sensor is operable to sense a touch force applied by the user to the display and calculate coordinates of a touch by the user, and further operable to analyze the touch force and, based on the touch force, select a tap command or a press command based on a predetermined criteria; a projector communicatively coupled to the processor, the projector being operable to project a data onto a viewing surface external to the device, the data including a virtual keyboard operable to input commands to the processor and one or more of the following: the notification of the external device, time, data requested by the user, a caller name, a text message, a reminder, a social media alert, an email, and a weather alert; a timepiece unit communicatively coupled to the processor and configured to provide time data; one or more activity tracking sensors communicatively coupled to the processor to track activity of the user, wherein the one or more activity tracking sensors are operable to track snoring and, based on tracking of the snoring, produce an alarm to break snoring; a memory unit communicatively coupled to the processor; a communication circuit communicatively coupled to the processor and operable to connect to a wireless network and communicate with the external device; a housing adapted to enclose at least the processor, the display, the one or more activity tracking sensors, the memory unit, and the communication circuit; an input unit communicatively coupled to the processor, wherein the input unit extends from the housing and is configured to perform one or more of a rotational motion and a linear motion, wherein the one or more motions are operable to input commands to the processor; and a band adapted to attach to the housing and to secure the wearable personal digital device for facilitating mobile device payments, personal use, and health care on a user body, wherein the device comprises a wristwatch; one or more biometric sensors disposed within the band and operable to sense one or more biometric parameters of the user, wherein based on detection that the one or more of the biometric parameters exceed predetermined limits, the one or more biometric sensors are configured to produce the alarm, wherein the one or more biometric sensors include lenses operable to use infrared light-emitting diodes (LED) and visible-light LEDs to sense a heart rate of the user, wherein the one or more biometric sensors include a skin contact sensor data engine, the skin contact sensor data engine being operable to monitor a user electrocardiogram and a user heart rate, the user electrocardiogram and the heart rate being identification and personal data of the user, wherein the skin contact sensor data engine is operable to prompt the user to enter a personal identification number and associate the personal identification number with both the user electrocardiogram and the heart rate obtained after the device has been secured to a wrist of the user, the obtained electrocardiogram and heart rate being stored in the memory unit as a reference electrocardiogram and reference heart rate; and a thermal infrared (IR) measurement of sensor is used to investigate the potential of cancer detection; an adhesive sensor system worn on the skin that automatically detects human falls and fatal diseases, the sensor, which consists of a tri-axial accelerometer, a microcontroller and a Bluetooth Low Energy transceiver, can be worn anywhere on a human body to detect a specific biological analyte by essentially converting a biological entity into an electrical signal that can be detected and analyzed by using of biosensor in cancer and other fatal diseases detection and monitoring; a haptic touch control actuator operable to produce a haptic feedback in response to one or more events, the one or more events including receiving of the alert, receiving of a notification, a confirmation, movement of the wearable personal digital device for facilitating mobile device payments, personal use, and health care, receiving of the user input, and sensing of the one or more biometric parameters, the haptic feedback being sensed by the user body, wherein the haptic feedback includes a plurality of feedback types, each of the one or more events being associated with one of the plurality of feedback types; a battery disposed in the housing; a magnetic inductive charging unit being operable to magnetically connect to the housing and wirelessly connect to the battery, wherein the magnetic inductive charging unit is operable to wirelessly transfer energy to the battery, wherein the magnetic inductive charging unit is integrated into the housing; wherein the user input is received using one or more of the display, the input unit, and the natural language user interface; wherein the device further comprises a camera communicatively coupled to the processor, operable to capture an optical code including one or more of the following: a linear dimensional barcode, a two-dimensional barcode, a snap tag code, and a Quick Response (QR) code; wherein the processor is further operable to read the code to obtain one or more of a product information and a merchant information encoded in the code and based on the merchant information, initiate a payment transaction, wherein the payment transaction is performed by sending payment data by the NFC unit to a merchant using a NFC unit to a merchant using the NFC; wherein the device further comprises a swipe card reader communicatively coupled to the processor and operable to read data of a payment card swiped therethrough, the data being transmitted to the processor or the external device; and wherein the processor of the device is further operable to: generate, based on the user payment data and the user personal data, a code encoding the user payment data and the user personal data, the user payment data and the user personal data being stored in the memory unit; prompt the user to touch the display to scan user fingerprints; determine a heart rate and electrocardiogram of the user; compare the scanned user fingerprints, determined heart rate and determined electrocardiogram with reference fingerprints stored in the memory unit, the reference heart rate, and the reference electrocardiogram; detect a match of the scanned user fingerprints with the reference fingerprints, the determined heart rate with the reference heart rate, and the determined electrocardiogram with the reference electrocardiogram; and wherein the processor, after the detecting of the matches, provides the code via the display of the device to a merchant digital device for performing a payment transaction; and wherein the processor, upon performing the payment transaction, provides a payment confirmation to the user, the payment confirmation being provided using the haptic feedback, and the payment transaction being completed by NFC.
KR2017139644A;Non-transitory computer-readable storage medium storing instructions for device voice control, includes instructions for performing task and disambiguating spoken user input The non-transitory computer-readable storage medium includes instructions for receiving (905) a spoken user input. The spoken user input is interpreted (910) to derive representation of user intent. The determination is made (915) whether task is identified based on representation of user intent. The task is performed (920), in accordance with determination that task is identified based on representation of user intent. The spoken user input is disambiguated (925), in accordance with determination that task can not be identified based on representation of user intent. INDEPENDENT CLAIMS are included for the following:a method for device voice control; andan electronic device. Non-transitory computer-readable storage medium storing instructions for device voice control. The speech recognition is improved for better recognition of non-standard terms and more reliable interpretation of spoken user inputs. The drawing shows a flowchart for a process for performing device voice control. 905Step for receiving a spoken user input910Step for interpreting the spoken user input915Step for determining whether the task is identified920Step for performing the task925Step for disambiguating the spoken user input A method, wherein: The method comprises the step of receiving the voice user input by the electronic device as the method; the step in order to determine the expression of the user purpose, of interpreting the voice user input; the step that the task determines the paper discriminated based on the expression of the user purpose; the step of performing the task according to the determination that the task can be discriminated based on the expression of the user purpose; and the step of doing the voice user input according to the determination that the task cannot be discriminated based on the expression of the user purpose with the formulation (disambiguating).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160337413A1;Method for conducting online meeting using natural language processing for automated content retrieval, involves providing links to participant during meeting to enable participant to view and share matching stored content items in meeting The method involves continually recognizing and analyzing speech of participants in an online meeting to extract participant speech content by a processing circuitry. An enterprise content management system is continually searched using the extracted participant speech content and content description information to identify matching stored content items. Links or other controls are dynamically provided to the participant during the online meeting to enable the participant to view and selectively share the matching stored content items in the online meeting. INDEPENDENT CLAIMS are also included for the following:online meeting server equipmenta computer program product comprising a set of instructions for conducting an online meeting using natural language processing for automated content retrieval. Method for conducting an online meeting using natural language processing for automated content retrieval. The method allows a technique to make the online meetings productive and effective by use of a feature for enabling a meeting system to proactively provide information relevant to topics to be discussed at the online meeting. The drawing shows a schematic a block diagram of a computer system. 11-1, 11-2Server computer12Network14User device24Biometric matching infrastructure server26Biometric matching infrastructure database28Document source A computer-implemented method of conducting an online meeting, comprising: maintaining, by processing circuitry, an enterprise content management system storing content description information describing computer-renderable stored content items; continually recognizing and analyzing, by the processing circuitry, speech of one or more participants in the online meeting to extract participant speech content; continually searching the enterprise content management system using extracted participant speech content and the content description information to identify matching stored content items, and dynamically providing links or other controls to the participant during the online meeting to enable the participant to view and selectively share the matching stored content items in the online meeting.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160351190A1;Non-transitory computer-readable storage medium storing instructions for device voice control, includes instructions for performing task and disambiguating spoken user input The non-transitory computer-readable storage medium includes instructions for receiving (905) a spoken user input. The spoken user input is interpreted (910) to derive representation of user intent. The determination is made (915) whether task is identified based on representation of user intent. The task is performed (920), in accordance with determination that task is identified based on representation of user intent. The spoken user input is disambiguated (925), in accordance with determination that task can not be identified based on representation of user intent. INDEPENDENT CLAIMS are included for the following:a method for device voice control; andan electronic device. Non-transitory computer-readable storage medium storing instructions for device voice control. The speech recognition is improved for better recognition of non-standard terms and more reliable interpretation of spoken user inputs. The drawing shows a flowchart for a process for performing device voice control. 905Step for receiving a spoken user input910Step for interpreting the spoken user input915Step for determining whether the task is identified920Step for performing the task925Step for disambiguating the spoken user input A non-transitory computer-readable storage medium storing one or more programs, the one or more programs comprising instructions, which when executed by one or more processors of an electronic device, cause the electronic device to: receive a spoken user input; interpret the spoken user input to derive a representation of user intent; determine whether a task may be identified based on the representation of user intent; in accordance with a determination that a task may be identified based on the representation of user intent, perform the task; and in accordance with a determination that a task may not be identified based on the representation of user intent, disambiguate the spoken user input.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160133254A1;Computing system, has cross- source search component for searching multiple different information sources based on current context and arguments in linguistic processing result to identify content on which action is performed The system has a user interface mechanism for receiving a linguistic processing result (146) indicative of an intent and a set of arguments (208, 210) recognized in an utterance. A context identification system identifies a current context of the system. An action identifier identifies action based on the current context and the intent in the linguistic processing result. A cross- source search component searches multiple different information sources based on the current context and the arguments in the linguistic processing result to identify content on which the action is performed. The action identifier identifies the action of send, open, schedule a meeting, create, attach, email, set a reminder, share, present, launch and display. An INDEPENDENT CLAIM is also included for a method for operating a computing system. Computing system. Uses include but are not limited to desktop computer, laptop computer, tablet computer, smart phone, phablet, cell phone, multimedia player, and personal digital assistant. The system generates textual representation of the utterance and identifies the intent and the arguments in the linguistic processing result in an efficient manner. The drawing shows a block diagram of a speech processing result. 146Linguistic processing result206Intent identifier208, 210Arguments212Textual representation214Item A computing system, comprising: a user interface mechanism that receives an utterance, sends the utterance to a linguistic processing system and receives a linguistic processing result indicative of an intent and a set of arguments recognized in the utterance; a context identification system that identifies a current context of the computing system; an action identifier that identifies an action based on the current context and based on the intent in the linguistic processing result; and a cross-source search component that searches multiple different information sources based on the current context and based on the set of arguments in the linguistic processing result to identify content on which the action is to be performed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN107077503A;Computing system, has cross- source search component for searching multiple different information sources based on current context and arguments in linguistic processing result to identify content on which action is performed The system has a user interface mechanism for receiving a linguistic processing result (146) indicative of an intent and a set of arguments (208, 210) recognized in an utterance. A context identification system identifies a current context of the system. An action identifier identifies action based on the current context and the intent in the linguistic processing result. A cross- source search component searches multiple different information sources based on the current context and the arguments in the linguistic processing result to identify content on which the action is performed. The action identifier identifies the action of send, open, schedule a meeting, create, attach, email, set a reminder, share, present, launch and display. An INDEPENDENT CLAIM is also included for a method for operating a computing system. Computing system. Uses include but are not limited to desktop computer, laptop computer, tablet computer, smart phone, phablet, cell phone, multimedia player, and personal digital assistant. The system generates textual representation of the utterance and identifies the intent and the arguments in the linguistic processing result in an efficient manner. The drawing shows a block diagram of a speech processing result. 146Linguistic processing result206Intent identifier208, 210Arguments212Textual representation214Item A computing system, comprising: a user interface mechanism for receiving the utterance, the utterance is sent to the language processing system and receiving an indication the intention in the utterance recognition out of and a set of argument of a language processing result and a context recognition system. current context action identifier identifying the computing system, based on the current context and based on the intention in the language processing result identifies action, a searching unit based on the current context and based on the set of arguments in the language processing result to search a plurality of different information sources. to identify the execution content of the action component, and a motion controlled system for identifying out the content of the action is taken.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9646611B2;Computing system, has cross- source search component for searching multiple different information sources based on current context and arguments in linguistic processing result to identify content on which action is performed The system has a user interface mechanism for receiving a linguistic processing result (146) indicative of an intent and a set of arguments (208, 210) recognized in an utterance. A context identification system identifies a current context of the system. An action identifier identifies action based on the current context and the intent in the linguistic processing result. A cross- source search component searches multiple different information sources based on the current context and the arguments in the linguistic processing result to identify content on which the action is performed. The action identifier identifies the action of send, open, schedule a meeting, create, attach, email, set a reminder, share, present, launch and display. An INDEPENDENT CLAIM is also included for a method for operating a computing system. Computing system. Uses include but are not limited to desktop computer, laptop computer, tablet computer, smart phone, phablet, cell phone, multimedia player, and personal digital assistant. The system generates textual representation of the utterance and identifies the intent and the arguments in the linguistic processing result in an efficient manner. The drawing shows a block diagram of a speech processing result. 146Linguistic processing result206Intent identifier208, 210Arguments212Textual representation214Item A computing system, comprising: a processor; and memory storing instructions executable by the at least one processor, wherein the instructions, when executed, configure the computing system to provide: a user interface mechanism configured to receive an utterance, send the utterance to a linguistic processing system, and receive a linguistic processing result indicative of an intent and a set of arguments recognized in the utterance; a context identification system configured to identify a current context of the computing system; an action identifier configured to identify an action based on the current context and based on the intent in the linguistic processing result; and a cross-source search component configured to identify content on which the action is to be performed by searching multiple different information sources based on the current context and based on the set of arguments in the linguistic processing result.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9692865B2;Mobile terminal i.e. mobile phone, has controller displaying set of candidates to be analyzed based on command, when no single contact is present, receiving input according to candidates, and executing call relating operation based on input The terminal (100) has a controller for displaying an indicator on a display unit (151) indicating that first voice input is recognized by the terminal. The controller executes a call relating operation, when a single contact is present in a phonebook that matches a voice command in the first voice input. The controller displays a set of candidates to be analyzed based on the voice command, when no single contact is present. The controller receives second input according to the candidates, and executes the call relating operation based on the second input. An INDEPENDENT CLAIM is also included for a method for controlling a mobile terminal having a display. Mobile terminal i.e. mobile phone (from drawings). The controller controls menus relating to specific functions or services even by a beginner user through voice command while providing help information with respect to input of the voice command according to an operation state or an operation mode. The terminal significantly improves a voice recognition rate by specifying a domain for voice recognition into a domain relating to specific menus or services. The drawing shows a front perspective view of a mobile terminal. 100Mobile terminal121Camera151Display unit152Audio output module215Keypad A mobile terminal, comprising: a wireless communication unit configured to provide wireless communication; a memory configured to store at least one database (DB); a display; and a controller configured to: activate a mode for voice recognition in response to a touch input to a soft button displayed on the display or to a hard button on the mobile terminal, receive a first voice input associated with an executable menu relating operation of the mobile terminal, display an indicator on the display indicating the first voice input is being recognized by the mobile terminal, determine a meaning of a voice command in the first voice input based on information stored in the at least one database (DB), display information corresponding to the determined meaning, receive a second voice input associated with a user's confirmation about the displayed information, if the received second voice input matches an affirmative received response, execute a menu relating operation corresponding to the displayed information, if the received second voice input matches a negative received response, perform a learning process based on a third voice input for learning the meaning of the voice command in the first voice input and execute a menu relating operation corresponding to the learned meaning, and update the at least one database (DB) to store the learned meaning according to the third voice input.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9564149B2;Method for e.g. performing user communication with information dialogue system, involves activating user input subsystem after formulated answer is displayed and/or reproduced after user inputs new-query or follow-up query The method involves activating a user input subsystem (BB) by a user (AA), and inputting a query. The query is received and converted into text by the input subsystem, and the text is obtained as a result of converting the query. The received text is processed by a dialogue module, and an answer to the query is formulated by the dialogue module. The answer is transmitted to the user, and the formulated answer is displayed and/or reproduced. The input subsystem is activated after the formulated answer is displayed and/or reproduced after the user inputs a new-query or a follow-up query. Method for performing user communication with an information dialogue system and organizing user interactions with the information dialogue system based on natural language. The drawing shows a schematic block diagram illustrating a method for performing user communication with an information dialogue system and organizing user interactions with an information dialogue system based on natural language. '(Drawing includes non-English language text)' AAUserBBUser input subsystemCCVoice recognition and recording componentDDKeyboardLLClient memory A method for user communications with an information dialogue system, the method comprising: activating, by a user device, a user input subsystem associated with the user device upon a request entered by a user; receiving, by the user input subsystem, the request; converting, by the user input subsystem, the request into text; sending, by the user input subsystem, the text obtained as a result of the converting of the request to a dialogue module associated with the user device; upon sending the text, deactivating the user input subsystem; processing the text by the dialogue module; forming, by the dialogue module, a response to the request, the response including an instruction for activating the user input subsystem, the instruction including at least metadata and a time period required for the user to review the response; sending, by the dialogue module, the response to the user; reproducing the response by a voice generation and reproduction subsystem associated with the user device; upon the reproducing of the response, determining, based on the metadata contained in the instruction, expiration of the time period required for the user to review the response; based on the determining, re-activating, by the user device, the user input subsystem; and receiving a further request or a clarification request, the further request or the clarification request being entered by the user.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170220963A1;Method for generating workflow, involves identifying at least one user-interface component from library of user-interface components and generating workflow including at least one user-interface component The method involves receiving (202) voice data defining a request to perform a task corresponding to operations of an enterprise. The voice data is converted (204) to text. At least one application programming interface associated with a first service defining an executable business function is identified (206) based on the text. At least one user-interface component is identified (208) from a library of user-interface components. A workflow including the at least one user-interface component is generated (210). The workflow is utilized by a user to complete the task. INDEPENDENT CLAIMS are included for the following:a non-transitory computer-readable medium encoded with instructions for generating workflows; anda system for generating workflows. Method for generating workflow to end users such as customers, partners or information technology (IT) developers. The user interacts with a graphical user-interface that allows the user to select the workflow and initiate execution. The speech to text recognition of services is improved. The drawing shows a flowchart the process for dynamically predicting workflows. 202Step for receiving voice data defining a request to perform a task corresponding to operations of an enterprise204Step for converting voice data to text206Step for identifying at least one application programming interface associated with a first service defining an executable business function208Step for identified at least one user-interface component210Step for generating workflow including the at least one user-interface component A method for generating workflows comprising: receiving, at a computing device, voice data defining a request to perform a task corresponding to operations of an enterprise; converting, using the computing device, the voice data to text; based on the text, identifying, using the computing device, at least one application programming interface associated with a first service defining an executable business function; based on the at least one application programming interface, identifying, using the computing device, at least one user-interface component from a library of user-interface components, wherein the at least one user-interface component corresponds to a second service defining an executable business function capable of performing a portion of the task; and generating, at the computing device, a workflow including the at least one user-interface component, wherein the workflow may be utilized by a user to complete the task.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20150254061A1;Method for user training of information dialogue system, involves formulating answer to training query as voice reply and/or text, and/or action carried out by system The method involves activating a user input subsystem (BB) by a user (AA). The user inputs a training query. The user's training query is received and transformed into text by user input subsystem. The text which is obtained as a result of converting training query is transmitted to a dialogue module (II). The training query text is processed by dialogue module. An answer to training query is formulated by dialogue module. The answer to training query is transmitted to user. The answer to training query is formulated as a voice reply and/or text, and/or an action carried out by system. Method for user training of information dialogue system based on natural language. The drawing shows a block diagram of the process for user training of the information dialogue system. (Drawing includes non-English language text) AAUserBBUser input subsystemCCVoice recognition and recording subsystemDDKeyboardIIDialogue module A method for user training of an information dialogue system, the method comprising: activating a user input subsystem; receiving, by the user input subsystem, a training request, the training request being entered by a user; converting, by the user input subsystem, the training request of the user into a text; sending the text of the training request obtained as a result of the converting to a dialogue module; processing, by the dialogue module, the text of the training request; forming, by the dialogue module, a response to the training request, wherein the response to the training request is formed as one or more of the following: a voice cue, a response text, and an action performed by the user input subsystem; and sending the response to the training request to the user.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20150255089A1;Method for e.g. performing user communication with information dialogue system, involves activating user input subsystem after formulated answer is displayed and/or reproduced after user inputs new-query or follow-up query The method involves activating a user input subsystem (BB) by a user (AA), and inputting a query. The query is received and converted into text by the input subsystem, and the text is obtained as a result of converting the query. The received text is processed by a dialogue module, and an answer to the query is formulated by the dialogue module. The answer is transmitted to the user, and the formulated answer is displayed and/or reproduced. The input subsystem is activated after the formulated answer is displayed and/or reproduced after the user inputs a new-query or a follow-up query. Method for performing user communication with an information dialogue system and organizing user interactions with the information dialogue system based on natural language. The drawing shows a schematic block diagram illustrating a method for performing user communication with an information dialogue system and organizing user interactions with an information dialogue system based on natural language. '(Drawing includes non-English language text)' AAUserBBUser input subsystemCCVoice recognition and recording componentDDKeyboardLLClient memory A method for user communications with an information dialogue system, the method comprising: activating a user input subsystem upon a request entered by the user; receiving the request by the user input subsystem; converting, by the user input subsystem, the request into text; sending the text obtained as a result of the converting of the request to a dialogue module; processing the text by the dialogue module; forming, by the dialogue module, a response to the request; sending the response to the user; reproducing the response; upon the reproducing of the response, re-activating the user input subsystem; and receiving a further request or a clarification request, the further request or the clarification request being entered by the user.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9542956B1;Method for responding to human spoken audio, involves selecting information source Application Programming Interface (API) from multiple APIs, for processing second audio command The method involves continuously listening for a first audio command. The second audio command is listened continuously upon receiving audio initiating command and processing it on intelligent assistant device. An information source API is selected from multiple APIs, for processing the second audio command. A response to the second audio command is received from the selected information source API and is transmitted to the intelligent assistant device. The response is outputted (170) through the intelligent assistant device. INDEPENDENT CLAIMS are included for the following:device for responding to human spoken audio; andsystem for responding to human spoken audio. Method for responding to human spoken audio. The API response can be saved and paired with the query data and saved in a cache for efficient lookup. The drawing shows a flow chart illustrating a system for processing human spoken audio. 105Step for receiving user spoken question107Step for outputting response125Step for transmitting audio130Step for transmitting user ID155Step for formatting response A method, comprising: continuously listening, via a microphone on an intelligent assistant device, for a first audio command, the first audio command being an audio initiating command; upon receiving the audio initiating command and processing it on the intelligent assistant device, continuously listening, via the intelligent assistant device, for a second audio command; transmitting the second audio command from the intelligent assistant device to a command processing server; selecting, by the command processing server, at least one information source Application Programming Interface (API) from a plurality of APIs, for processing the second audio command, the second audio command having been converted from speech-to-text, wherein the selecting is based on at least one of: context, category, meaning, and keywords in the converted text of the second audio command; transmitting, by the command processing server, the converted text of the second audio command to the selected at least one information source API for processing; receiving at the command processing server, a response to the second audio command from the selected at least one information source API, the response provided via a notifications server; transmitting the response from the command processing server to the intelligent assistant device; and outputting the response via the intelligent assistant device.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PH12017550012A1;Computing device for headlessly completing a task of an application in the background of a digital personal assistant, comprises a processing unit, where computing device is configured with a speech-controlled digital personal assistant The computing device (130) comprises a processing unit, a memory, and microphones (150). The computing device is configured with a speech-controlled digital personal assistant (120), where the operations comprises receiving speech input generated by a user via the microphones. The speech recognition is performed using the speech input to determine a spoken command, where the spoken command comprises a request to perform a task of a third-party application (110). The task (112) is identified using a data structure (140) that defines tasks of third-party applications invokable by spoken command. INDEPENDENT CLAIMS are included for the following:a method for headlessly completing a task of an application in the background of a digital personal assistant; anda computer-readable storage medium for storing computer-executable instructions for causing a computing device to perform operations for completing a task of a voice-enabled application. Computing device for headlessly completing a task of an application in the background of a digital personal assistant. The voice-enabled application executes the task as a background task and within a context of the voice-controlled digital personal assistant without a user interface of the voice-enabled application surfacing. The responses provided to the user from the voice-controlled digital personal assistant are selected from display text, text-to-speech, deep-link user resource identifier, Hypertext markup language, list template, physical address, or telephone number. The drawing shows a schematic block diagram of a system for headlessly completing a task of an application. 110Third-party application112Task120Speech-controlled digital personal assistant130Computing device140Data structure150Microphones ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RU2530268C2;Method for user training of information dialogue system, involves formulating answer to training query as voice reply and/or text, and/or action carried out by system The method involves activating a user input subsystem (BB) by a user (AA). The user inputs a training query. The user's training query is received and transformed into text by user input subsystem. The text which is obtained as a result of converting training query is transmitted to a dialogue module (II). The training query text is processed by dialogue module. An answer to training query is formulated by dialogue module. The answer to training query is transmitted to user. The answer to training query is formulated as a voice reply and/or text, and/or an action carried out by system. Method for user training of information dialogue system based on natural language. The drawing shows a block diagram of the process for user training of the information dialogue system. (Drawing includes non-English language text) AAUserBBUser input subsystemCCVoice recognition and recording subsystemDDKeyboardIIDialogue module ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170339340A1;Image capture device e.g. digital camera has processor that determines whether image data is captured by user and update user preference data based on image data is captured or not The device has a processor that extracts (205) a visual representation of target subject from database. The target subject in an environment viewed through camera lens is detected (206) by comparing visual representation of target subject with objects appearing in environment. The image capture function is applied (207) to target subject detected in environment. Determination is made on whether image data is captured by user. The user preference data is updated, that based on whether image data is captured (208) or not. INDEPENDENT CLAIMS are included for the following:a computer-based method of learning image capture device user preferences; anda computer program product for learning image capture device user preferences. Image capture device such as standalone camera e.g. digital camera, tablet, smartphone or tablet computer. The camera tuning advisor advise a user desiring a higher or lower temperature of an image to increase or decrease shutter speed because a stated temperature goal is converted to a shutter speed goal. The setting feature for increasing temperature increase a first quality feature and decrease a second quality feature thus results in a higher temperature image captured. The camera self-tuning is performed automatically by the camera and includes camera provides setting adjustment advice to user through camera tuning advisor. The drawing shows a flow diagram illustrating for a process of capturing image data. 201Step for receiving voice command from user205Step for extracting visual representation of target subject from database206Step for detecting target subject in an environment viewed through camera lens207Step for applying image capture function to target subject detected in environment208Step for capturing user preference data An image capture device, comprising: a camera lens configured to capture image data; a microphone configured to receive a voice command from a user; a network interface configured to establish a network connection; a memory storing a computer program and user preference data; and a processor configured to execute the computer program, wherein the computer program is configured to: identify a target subject and an image capture function based on the voice command and the user preference data; search a first database for the target subject, wherein the first database is stored on the memory; search a second database for the target subject when the target subject is not found in the first database, wherein the second database is located remote from the image capture device and is accessed via the network connection; extract a visual representation of the target subject from the first database or the second database upon finding the target subject in the first database or the second database; detect the target subject in an environment viewed through camera lens by comparing the visual representation of the target subject with objects appearing in the environment; apply the image capture function to the target subject detected in the environment; determine if image data is captured by the user; and update the user preference data based on whether the image data is captured or not.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9634855B2;User interface device for use with e.g. cell phone of e.g. TV used by elderly or lonely person for social interaction, has audio visual output presenting anthropomorphic object controlled by automated processor The device i.e. personal interface device has an audio visual information input to receive information sufficient to determine a topic of interest to a user and a query by a user depending on the received audio visual information. An audio visual output presents an anthropomorphic object controlled by an automated processor for conveying information of interest to the user depending on the determined topic of interest and the query with an audio visual telecommunication interface. An automated processor controls the anthropomorphic object with associated anthropomorphic mood. INDEPENDENT CLAIMS are also included for the following:a user interface system comprising an interactive interfacea method for using a User interface device. User interface device i.e. personal interface device for use with electronic system such as cell phone, smart phone i.e.    Apple iPhone  (RTM: Internet-connected multimedia smartphone), laptop computer and desktop computer of consumer electronics device such as TV, smart clock radio i.e.    Sony Dash  (RTM: fun device) and special purpose robot used by an elderly or lonely person for social interaction. Can also be used for electronic system such as    Blackberry  (RTM: handheld wireless electronic-mail device) smart phone, personal digital assistant (PDA) such as    Apple iPad  (RTM: tablet computer),    Apple iPod  (RTM: Portable media player) and    Amazon Kindle  (RTM: electronic-book reader) of consumer electronics device such as automobiles, set top box, stereo equipment, kitchen appliance, thermostats and heating, ventilation, and air conditioning (HVAC) equipment and laundry appliance. The device enables effectively detecting the user's facial features without discomfort to user. The drawing shows a front view of a cell phone.110Microphone120Camera130Speaker140Display150Image A user interface device comprising: an audio visual information input interface configured to receive information sufficient to determine at least one of a topic of interest to a user and a query from a user, dependent on at least received audio visual information; at least one memory configured to store an automatically generated user interest and knowledge profile based on at least prior interaction of the user interface device with the user; at least one automated processor, configured to: automatically generate at least one automated search engine query based on the at least one of the topic of interest and the query, communicate the at least one automated Internet search engine query to at least one remote search engine having a database of information items, through an automated computer communication network, receive responses from the at least one automated Internet search engine responsive to the at least one automated Internet search engine query identifying a source of information items, implement an interactive conversational agent, based on at least the stored user profile, the topic of interest or user query, and the identified information items; and  at least one audio visual output interface configured to present the interactive conversational agent dynamically controlled by the at least one automated processor, conveying the information of interest to the user; and an audio visual telecommunication interface.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20150340033A1;System for processing and interpreting natural language such as interpretations of user utterances, has processor that generates second response for presentation to user based on second speech processing results The system has a processor that generates second speech processing results using the second audio data, the context information, and a context interpretation rule, such that the context interpretation rule relates to replacing slot value or an intent of the semantic representation of the first utterance with a slot value or an intent of a semantic representation of the second utterance, and such that the context rule is based on the semantic representation of the first response. A second response is generated for presentation to the user based on the second speech processing results. INDEPENDENT CLAIMS are included for the following:a computer-implemented method for processing and interpreting natural language; andnon-transitory computer-readable storage for processing and interpreting natural language. System for processing and interpreting natural language such as interpretations of user utterances. Advantageously, interpreting current utterances within the context of prior turns in a multi-turn interaction can help solve ambiguities without requiring additional user input, and can help avoid initiating an incorrect action based on an interpretation of an utterance in isolation when the correct action is determinable in the context of the entire multi-turn interaction. The processing is based on the context of the multi-turn interaction can allow more natural communication with a speech processing system and more robust dialog management without loss of context. The drawing shows a block diagram of the illustrative data flows between modules of a speech processing system during multi-turn dialog utterance processing using contextual information from previous turns. 102Microphone104Speaker206Context interpreter208Dialog manager212Context data store A system comprising: a computer-readable memory storing executable instructions; and one or more processors in communication with the computer-readable memory, wherein the one or more processors are programmed by the executable instructions to at least: obtain first audio data regarding a first utterance of a user; generate first speech processing results based at least partly on the first audio data, the first speech processing results comprising a semantic representation of the first utterance; generate a first response for presentation to the user based at least partly on the first speech processing results; store context information comprising the semantic representation of the first utterance and a semantic representation of the first response; obtain second audio data regarding a second utterance of the user; generate second speech processing results using the second audio data, the context information, and a context interpretation rule, wherein the context interpretation rule relates to replacing at least one of a slot value or an intent of the semantic representation of the first utterance with at least one of a slot value or an intent of a semantic representation of the second utterance, and wherein the context rule is based at least partly on the semantic representation of the first response; and generate a second response for presentation to the user based at least partly on the second speech processing results.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170011742A1;Intent understanding apparatus integrated as voice interface used in e.g. car navigation apparatus, has correction unit that selects intent understanding result from intent understanding result candidates based on last score The understanding apparatus (1) has intent understanding unit (7) that outputs score which estimates intent of user speech based on morphological column, and represents degree of intent understanding result candidates and plausibility from morphological column. A calculation unit (11) calculates weight for every intent understanding result candidate. The last score is calculated by amending score of intent understanding result candidate using weight. A correction unit (12) selects intent understanding result (13) from intent understanding result candidates based on last score. An INDEPENDENT CLAIM is included for the intent understanding method. Intent understanding apparatus integrated as voice interface used in mobile telephone and car navigation apparatus. The user intent is understood correctly using input voice. The drawing shows a block diagram of the intent understanding apparatus. (Drawing includes non-English language text) 1Intent understanding apparatus7Intent understanding unit11Weight calculation unit12Intent understanding correction unit13Intent understanding result 1 An intent understanding device, comprising: a voice recognizer that recognizes one speech spoken in a natural language by a user, to thereby generate plural voice recognition results of highly ranked recognition scores; a morphological analyzer that converts the respective voice recognition results into morpheme strings; an intent understanding processor that estimates an intent about the speech by the user on the basis of each of the morpheme strings, to thereby output from each one of the morpheme strings, one or more candidates of intent understanding result and scores indicative of degrees of likelihood of the candidates and generate the candidates of intent understanding result in descending order of likelihoods of the plural voice recognition results; a weight calculator that calculates respective weights for the candidates of intent understanding result; and an intent understanding corrector that corrects the scores of the candidates of intent understanding result, using the weights, to thereby calculate their final scores, and then selects the candidate of intent understanding result with the final score that satisfies a preset condition first, as the intent understanding result.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9401142B1;Method for validating natural language content using crowdsourced validation job, involves obtaining transcription pair provided with natural language content by computer system, and validation score is assigned to transcription pair The method involves obtaining a transcription pair provided with the natural language content and text by a computer system. The natural language content is provided with the audio content received from multiple audio input components. A crowdsourced validation job to be performed at multiple validation devices is created. A validation score is assigned to the transcription pair based on the votes received from the validation devices. An INDEPENDENT CLAIM is included for a system for validating the natural language content using crowdsourced validation jobs. Method for validating the natural language content using crowdsourced validation jobs. The method involves obtaining a transcription pair provided with the natural language content and text by a computer system, and hence enhances the accuracy level, ensures accurately representating the voice data in the unvalidated transcription job and effectively transcribe the speech to the text without the noise. The drawing shows a schematic view of the natural language processing environment. 100Natural language processing environment108Network112Transcription validation server114Transcription engine116Validation engine A computer-implemented method, the method being implemented in a computer system having one or more physical processors programmed with computer program instructions that, when executed by the one or more physical processors, program the computer system to perform the method, the method comprising: obtaining, by the computer system, a transcription pair comprising natural language content and text, wherein the natural language content comprises audio content received from one or more audio input components, and wherein the text corresponds to a transcription of the natural language content; creating, by the computer system, a first crowdsourced validation job to be performed at one or more first validation devices, the first crowdsourced validation job providing the one or more first validation devices with first instructions for a crowd user to provide a determination of whether or not the text is an accurate transcription of the natural language content; causing, by the computer system, the first crowdsourced validation job to be provided to the one or more first validation devices; receiving, by the computer system, from each of the one or more first validation devices, a vote representing the determination of whether or not the text is an accurate transcription of the natural language content; assigning, by the computer system, a validation score to the transcription pair based, at least in part, on votes received from the one or more first validation devices with respect to the transcription pair; determining, by the computer system, whether or not the one or more first validation devices agreed the text is an accurate transcription of the natural language content; and determining, by the computer system, whether to provide the transcription pair to one or more second validation devices.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US8788269B2;Method for satisfying specified intents based on e.g. multi-modal requests on tablet, involves monitoring audio signals received through audio interface for speech requests from user to be processed using speech understanding functionality The method (200) involves determining that entities in visual content are selected in accordance with an explicit scoping command from a user (202), where the content is displayed on a display of a processing system. The speech understanding functionality of the system is automatically activated (204) by using a processor of the system when the entities are selected. Audio signals received through an audio interface of the processing system are automatically monitored (206) for speech requests from the user to be processed using the understanding functionality when the entities are selected. INDEPENDENT CLAIMS are also included for the following:a processing system comprising a displaya computer program product comprising a set of instructions to perform a method for satisfying specified intents. Method for satisfying specified intents based on multi-modal requests on a processing system e.g. tablet, mobile phone and desktop computer. Uses include but are not limited to other requests e.g. speech requests, text commands, tactile commands and visual commands. The method enables quickly satisfying the specified intents based on the multi-modal requests for the user in a simple manner without distracting the user from the task so as to achieve quicker completion of tasks. The drawing shows a flowchart illustrating a method for satisfying specified intents based on multimodal requests.200Method for satisfying specified intents based on multi-modal requests202Step for determining that entities in visual content are selected in accordance with explicit scoping command from user204Step for automatically activating speech understanding functionality of processing system206Step for automatically monitoring audio signals received through audio interface of processing system208Step for automatically monitoring tactile interface of processing system A method comprising: determining that one or more entities in visual content, which is displayed on a display of a processing system, are selected in accordance with an explicit scoping command from a user, the explicit scoping command identifying the one or more entities in the visual content to explicitly indicate a scope of an interaction between the user and the processing system; turning on speech understanding functionality of the processing system, using at least one processor of the processing system, in response to determining that the one or more entities are selected, the speech understanding functionality enabling the processing system to understand natural language requests; and automatically monitoring audio signals received via an audio interface of the processing system for speech requests from the user to be processed using the speech understanding functionality in response to determining that the one or more entities are selected.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN105989840A;Method for hybrid processing in natural language voice services environment, involves resolving multi-modal interaction at electronic device based on information contained in multiple messages received from virtual router The method involves detecting multi-modal interaction at an electronic device e.g. mobile phone. Multiple messages containing information related to the multi-modal interaction is communicated to a virtual router in communication with the electronic device. Multiple messages containing information related to intent of the multi-modal interaction is received by the electronic device. The multi-modal interaction at the electronic device is resolved based on the information contained in multiple messages received from the virtual router. INDEPENDENT CLAIMS are also included for the following:an electronic device for hybrid processing in a natural language voice services environmenta virtual router for hybrid processing in a natural language voice services environment. Method for hybrid processing in natural language voice services environment using an electronic device e.g. microphone, personal navigation device, mobile phone, Voice over Internet Protocol (VoIP) node, personal computer, media device, embedded device and server, and a virtual router (all claimed). The method enables performing hybrid processing in the natural language voice services environment using the electronic device and/or the virtual router without degrading performance in an efficient manner. The drawing shows a block diagram of a voice-enabled device for hybrid processing in a natural language voice services environment.100Voice-enabled device112Input devices120Automatic speech recognizer130Conversational language processor134Applications A method for mixing processing in a natural language voice service environment, the method is implemented in a computer system, the computer system comprising one or more physical processors programmed with computer program instructions, the computer program instructions, when executed by the one or more physical processors, the computer system programmed to perform the method, the method comprising: the computer system receiving a plurality of audio coding, wherein the plurality of audio encoding at least comprises encoding first audio and second audio encoding. the first audio coding corresponding to the natural language of the user captured by the first electronic device sound, said second audio encoding corresponding to the natural language of the user to the second electronic device the captured sound, a first value of the computer system determining the first audio encoding of audio characteristics of the computer system to determine a second value of the audio characteristic of the second audio coding, the computer system selecting the first audio coding based on said first value and said second value or said second audio encoding; and said computer system based on said first audio encoding and the selected one of the second audio coding to obtain the purpose of the natural language utterance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9502027B1;Method for processing speech, involves reparsing further natural language speech input with automated statistical processor in conjunction with previously parsed natural language speech input The method involves providing an automated speech processing system. A natural language speech input generated by a microphone is received. A determination is made if the semantic parsing of the received natural language speech input corresponds to a single non-excluded command outcome. The further natural language speech input is reparsed (S122) with the automated statistical processor in conjunction with previously parsed natural language speech input. The determination process is exited if an abort, fail or cancel condition is detected in the natural language speech input. An INDEPENDENT CLAIM is included for a system for processing speech. Method for processing speech for controlling computer applications and processes. The Commands Dictionary (CD) can hold multiple entries or series of entries for the same command, effectively having the side effect of maintaining synonyms for commands in order to enhance the natural language processing capabilities of the system. The interaction between the logical command processor and the speech recognizer facilitates the process of building a command structure leading to a command input complete for command execution. The overall accuracy of the speech recognition process can be increased by analyzing the speech input under different criteria or even with different speech recognizers. The drawing shows a flow chart of a method for processing speech. S101Step for activating speech recognizerS102Step for loading grammarS103Step for prompting user for inputS105Step for receiving user inputS122Step for reparsing further natural language speech input A method for processing speech, comprising: providing an automated speech processing system having a command processor and at least one memory; receiving a natural language speech input generated by a microphone; semantically parsing the received natural language speech input using an automated statistical processor with respect to a plurality of predetermined command grammars in the automated speech processing system, the plurality of predetermined grammars defining mutually inconsistent command outcomes from the command processor, said semantically parsing selectively excluding predetermined command grammars having respective command outcomes inconsistent with the previously received natural language speech; determining: if the semantic parsing of the received natural language speech input corresponds to a single non-excluded command outcome according to a respective command grammar, and is complete for reliable processing by the command processor, then processing the command with the command processor, according to the single non-excluded command grammar and exiting said determining; if the received natural language speech input corresponds to a plurality of command outcomes, or is not complete for reliable processing according to a plurality of non-excluded predetermined command grammars, then: prompting a user for further natural language speech input dependent on at least one of the plurality of non-excluded predetermined command grammars, the prompting comprising feedback representing an identification of at least one command type putatively recognized, and information required to reduce correspondence to a plurality of non-excluded predetermined command grammars or to increase completeness, in dependence on a relationship of the previously received natural language speech input and at least one command grammar of the plurality of non-excluded predetermined command grammars, reparsing the further natural language speech input with the automated statistical processor in conjunction with previously parsed natural language speech input, and iterating said determining; and  if an abort, fail or cancel condition is detected in the natural language speech input, exiting said determining.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9171066B2;Distributed client-server arrangement for processing e.g. human generated text in smartphone, has ranking module for processing local interpretation candidates and remote interpretation candidates to determine final output interpretation The arrangement has a set of local data sources (203) e.g. music lists, stored on a mobile device (200). A local natural language understanding (NLU) match module (202) on the mobile device performs natural language processing of natural language input (201) with respect to local data sources to determine local interpretation candidates. A local NLU ranking module (206) on the mobile device processes the local interpretation candidates and remote interpretation candidates from a remote NLU server (204) to determine a final output interpretation (207) corresponding to the language input. An INDEPENDENT CLAIM is also included for a distributed natural language processing method. Distributed client-server arrangement for processing distributed natural language inputs such as human generated speech and human generated text, in a mobile device i.e. smartphone, to extract meaningful information from the natural language inputs. The arrangement can leverage on-device data for locally processing the data on the mobile device and remotely processing the data on the server using the resources by adopting NLU processing technique without occurrence of fault in confidentiality concern and sheer amount of data required to be uploaded and kept in-sync. The drawing shows a block diagram of a distributed natural language processing arrangement. 200Mobile device201Natural language input202Local NLU match module203Local data sources204Remote NLU server206Local NLU ranking module207Final output interpretation A method comprising: configuring a client device to: process one or more natural language inputs with respect to data sources stored on the client device to determine a first set of interpretation candidates for the one or more natural language inputs; and to communicate, to a server, results from processing the one or more natural language inputs with respect to the data sources stored on the client device; determining, by the server and based on the results from processing the one or more natural language inputs with respect to the data sources stored on the client device, a list of possible interpretation candidates for the one or more natural language inputs, the list comprising a second set of interpretation candidates for the one or more natural language inputs; ranking, by the server, the list of possible interpretation candidates; pruning, by the server, the list of possible interpretation candidates; constraining, by the server and based on pseudo data corresponding to the data sources located on the client device, the pruning to prevent at least one interpretation candidate of the second set of interpretation candidates from being pruned from the list of possible interpretation candidates; and communicating, by the server and to the client device, the second set of interpretation candidates for the one or more natural language inputs, for a final output interpretation of the one or more natural language inputs by the client device that comprises ranking a plurality of interpretation candidates comprising the first set of interpretation candidates and the second set of interpretation candidates.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9747279B2;System for determining user intent or goal for contextual language understanding, has processor which determines first user intent based on third set of entities and first set of carryover entities The system (100) has a processor which receives a first natural language input based on input from a first user. The processor identifies first set of entities in first natural language input. The processor identifies second set of entities in first response and receives second natural language input. The processor identifies third set of entities in second natural language input. The processor determines first user intent based on third set of entities and first set of carryover entities. The processor generates second response based on first user intent. An INDEPENDENT CLAIM is included for a method for contextual language understanding. System for determining user intent or goal for contextual language understanding. The systems provide for a more accurate, a more reliable, and a more efficient context carryover and goal tracking system. The drawing shows a block diagram of the context carryover and goal tracking system implemented at a client computing device for contextual language understanding. 100System102User104Client computing device106Network108Prediction system A system comprising: at least one processor; and a memory encoding computer executable instruction that, when executed by the at least one processor, cause the at least one processor to perform a method for contextual language understanding, the method comprising: receiving a first natural language input based on input from a first user; identifying a first set of entities in the first natural language input utilizing a schema; receiving a first response to the first natural language input based on the first set of entities, wherein the first response is generated by the system; identifying a second set of entities in the first response utilizing the schema; receiving a second natural language input; identifying a third set of entities in the second natural language input utilizing the schema; identifying a first set of carryover entities from any previous set of entities for carryover based on the third set of entities; determining a first user intent based on the third set of entities and the first set of carryover entities; and generating a second response based on the first user intent.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KR2015086313A;Distributed client-server arrangement for processing e.g. human generated text in smartphone, has ranking module for processing local interpretation candidates and remote interpretation candidates to determine final output interpretation The arrangement has a set of local data sources (203) e.g. music lists, stored on a mobile device (200). A local natural language understanding (NLU) match module (202) on the mobile device performs natural language processing of natural language input (201) with respect to local data sources to determine local interpretation candidates. A local NLU ranking module (206) on the mobile device processes the local interpretation candidates and remote interpretation candidates from a remote NLU server (204) to determine a final output interpretation (207) corresponding to the language input. An INDEPENDENT CLAIM is also included for a distributed natural language processing method. Distributed client-server arrangement for processing distributed natural language inputs such as human generated speech and human generated text, in a mobile device i.e. smartphone, to extract meaningful information from the natural language inputs. The arrangement can leverage on-device data for locally processing the data on the mobile device and remotely processing the data on the server using the resources by adopting NLU processing technique without occurrence of fault in confidentiality concern and sheer amount of data required to be uploaded and kept in-sync. The drawing shows a block diagram of a distributed natural language processing arrangement. 200Mobile device201Natural language input202Local NLU match module203Local data sources204Remote NLU server206Local NLU ranking module207Final output interpretation The dispersed natural language processing mode which comprises the local NLU ranking module on the mobile device for handling at least one remote interpretation candidates of the remote NLU server and local interpretation candidates it determines the final output interpretation according to the natural language input; the natural language understanding (NLU) matching module on the mobile device which is for the natural language input about the local data source with the natural language processing following it determines at least one local interpretation candidates; the local data source of 1 set (a set) stored in the mobile device as to the dispersed natural language processing mode.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KR1689818B1;Distributed client-server arrangement for processing e.g. human generated text in smartphone, has ranking module for processing local interpretation candidates and remote interpretation candidates to determine final output interpretation The arrangement has a set of local data sources (203) e.g. music lists, stored on a mobile device (200). A local natural language understanding (NLU) match module (202) on the mobile device performs natural language processing of natural language input (201) with respect to local data sources to determine local interpretation candidates. A local NLU ranking module (206) on the mobile device processes the local interpretation candidates and remote interpretation candidates from a remote NLU server (204) to determine a final output interpretation (207) corresponding to the language input. An INDEPENDENT CLAIM is also included for a distributed natural language processing method. Distributed client-server arrangement for processing distributed natural language inputs such as human generated speech and human generated text, in a mobile device i.e. smartphone, to extract meaningful information from the natural language inputs. The arrangement can leverage on-device data for locally processing the data on the mobile device and remotely processing the data on the server using the resources by adopting NLU processing technique without occurrence of fault in confidentiality concern and sheer amount of data required to be uploaded and kept in-sync. The drawing shows a block diagram of a distributed natural language processing arrangement. 200Mobile device201Natural language input202Local NLU match module203Local data sources204Remote NLU server206Local NLU ranking module207Final output interpretation A dispersed natural language processing system, wherein: The natural language processing system comprises local data sources of the set (a set) it is stored in the mobile device; the local natural language understanding matching module on the mobile device which is for the natural language input about local data sources with the natural language processing following in order to decide at least one local interpretation candidates; and the local NLU ranking module on the mobile device for handling at least one remote interpretation candidates from local interpretation candidates and remote NLU server in order to decide the final output interpretation corresponding to the natural language input (it is trained about the doctor (pseudo) - local data set in which the remote NLU server includes stored at least one data types with local data sources).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9570070B2;Multi-modal device interactions processing method for electronic devices in natural language voice services environment, involves routing request to electronic devices based on determined intent of multi-modal device interaction The method involves detecting a multi-modal device interaction. Context information related to the device interaction is extracted, where the extracted context information includes context related to a non-voice interaction. The context related to the non-voice interaction and a natural language utterance are combined. Intent of the device interaction is determined based on the combination of context related to the non-voice interaction and the natural language utterance. A request is routed to electronic devices (105) based on the determined intent of the device interaction. An INDEPENDENT CLAIM is also included for a system for processing multi-modal device interactions in a natural language voice services environment. Method for processing multi-modal device interactions between electronic devices in a natural language voice services environment. Uses include but are not limited to microphone, telematics device, personal navigation device, mobile phone, voice over Internet protocol (VoIP) node, personal computer, media device, embedded device, and server. The method enables determining the intent of the multi-modal device interaction based on the combination of context related to the non-voice interaction and the natural language utterance, so that the interaction with electronic devices is efficient, thus avoiding difficulties in utilizing the functionality of the devices. The method enables a voice-click module to enable a user to interact with the electronic devices in an intuitive and free-form manner, so that the user can provide various types of information to a system while seeking to initiate action, retrieve information and request content or services available in the system. The drawing shows a block diagram of a system for processing multi-modal device interactions in a natural language voice services environment.105Electronic devices105bNon-voice input device110Automatic speech recognizer120Conversational language processor160Data repositories A method for processing one or more multi-modal device interactions, received from a user, in a natural language voice services environment that includes a plurality of components that handle requests relating to the multi-modal device interact, the method being implemented on a computer system having one or more physical processors programmed with computer program instructions that, when executed by the one or more physical processors, program the computer system to perform the method, the method comprising: detecting, by the computer system, at least one multi-modal device interaction, wherein the multi-modal device interaction includes a non-voice interaction, from the user, with at least one of the plurality of components or an application associated with at least one of the plurality of components, and wherein the multi-modal device interaction further includes at least one natural language utterance, from the user, relating to the non-voice interaction; determining, by the computer system, a context relating to the non-voice interaction and a context relating to the natural language utterance; determining, by the computer system, an intent of the multi-modal device interaction based on the context relating to the non-voice interaction and the context of the natural language utterance; generating, by the computer system, a request based on the determined intent; obtaining, by the computer system, information indicating a capability of a component, from among the plurality of components, based on a constellation model that specifies the capabilities of each of the plurality of components; determining, by the computer system, that the component should handle the request based on the capability of the component; and routing, by the computer system, the request to the component.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US8694537B2;Method for processing natural language search queries in natural language query processor of e.g. user device, involves providing response to search query from aggregated natural language library to client device The method involves receiving natural language libraries from service providers (130) via a network (140), where each natural language library comprises natural language queries. An aggregated natural language library is generated from the received natural language libraries. Search query is received via the network. The search query is compared to the aggregated natural language library to determine the natural language query that corresponds to the search query. Response is provided to the search query from the aggregated natural language library to a client device. An INDEPENDENT CLAIM is also included for a natural language query processor (NLQP) comprising a memory. Method for processing natural language search queries in a NLQP (claimed) of a user device. Uses include but are not limited to desktop, personal computer (PC), laptop, notebook, game console i.e. X-box, music player, tablet, IPod (RTM: Portable media player), smartphone, automobile computer system and an internet enabled TV. The method enables matched libraries to be returned by the NLQP, so that a developer can choose to use the matched libraries to design/update a natural language query system to reduce duplicative effort by employing crowd sourced natural language libraries by the developer. The method enables customizing advertising, so that a search query history and information related to an end user is utilized to potentially increase effectiveness of advertising. The method enables the service providers to provide search results along with additional content that enhances search experience for the end users by utilizing a cloud computing network so as to enhance functionality without additional burden being placed upon the service providers. The drawing shows a schematic block diagram of a system for searching databases.100System for searching databases110Clients130Service providers140Network150Accounts A method for processing natural language queries, the method comprising: receiving two or more natural language libraries from service providers via a network, where each natural language library comprises: natural language queries for interacting with a client application; and responses for the natural language queries; generating an aggregated natural language library from the received natural language libraries, wherein the aggregated natural language library includes a plurality of natural language sub-libraries, wherein at least two of the natural language sub-libraries are associated with different client applications that each provide a service; receiving a search query via the network; comparing the search query to the aggregated natural language library to determine at least one natural language query that corresponds to the search query; and providing a response to the search query from the aggregated natural language library to a client device.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KR2016061071A;Method for performing voice recognition based on utterance variation, involves outputting word string in voice recognition module and converting into words by using language model database (DB) The method involves converting natural language audio signal into the electric signal, after the audio signal is input to the microphone and applying the utterance variation. The electric signal is converted into the digital signal in the pretreatment module. The digital signal is converted into the phoneme heat in the voice recognition module by using the acoustic model DB. A word string is output in the voice recognition module and is converted into words by using the language model DB. INDEPENDENT CLAIMS are included for the following:a computer-readable recording medium storing program for performing voice recognition; anda voice recognition unit. Method for performing voice recognition based on utterance variation. The voice recognition performance is improved efficiently, with reduced cost. The drawing shows a block diagram of the method for performing voice recognition based on utterance variation. 10Microphone20Pretreatment module200Voice recognition module300Language study module400Database The method for voice recognition which converts into the electric signal as to the method for voice recognition after the natural language audio signal is input to the microphone; and applies the utterance variation which comprises the phoneme series the word string output process of outputting the word string in the voice recognition module; the pronouncing dictionary DB and the word string generating process of converting into words by using the language model DB and producing the word string in the voice recognition module; the phoneme recognition process of converting the digital signal into the phoneme heat in the voice recognition module by using the acoustic model DB; the voice input procedure of converting the electric signal into the digital signal in the pretreatment module.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN106663424A;Intent understanding apparatus integrated as voice interface used in e.g. car navigation apparatus, has correction unit that selects intent understanding result from intent understanding result candidates based on last score The understanding apparatus (1) has intent understanding unit (7) that outputs score which estimates intent of user speech based on morphological column, and represents degree of intent understanding result candidates and plausibility from morphological column. A calculation unit (11) calculates weight for every intent understanding result candidate. The last score is calculated by amending score of intent understanding result candidate using weight. A correction unit (12) selects intent understanding result (13) from intent understanding result candidates based on last score. An INDEPENDENT CLAIM is included for the intent understanding method. Intent understanding apparatus integrated as voice interface used in mobile telephone and car navigation apparatus. The user intent is understood correctly using input voice. The drawing shows a block diagram of the intent understanding apparatus. (Drawing includes non-English language text) 1Intent understanding apparatus7Intent understanding unit11Weight calculation unit12Intent understanding correction unit13Intent understanding result An intention understanding device, comprising: sound recognizing part, one of which identifies the user using natural language speech, generating a plurality of speech recognition result, morpheme analyzing part, which the speech recognition result are converted into morpheme string, intention understanding part the estimating the intention of the utterance of the user according to said morpheme string from one of the morpheme string output more than one intention understanding result candidates and the fraction of the degree that the possibility of, weight calculation part which calculates the weight of each of the intention understanding result candidate; and intention understanding correction portion, which corrects the intention understanding result candidate using the weights of the score, calculating the final score, understanding result candidate selected from the intention understanding result from the intent based on the final score.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CN106796787A;System for processing and interpreting natural language such as interpretations of user utterances, has processor that generates second response for presentation to user based on second speech processing results The system has a processor that generates second speech processing results using the second audio data, the context information, and a context interpretation rule, such that the context interpretation rule relates to replacing slot value or an intent of the semantic representation of the first utterance with a slot value or an intent of a semantic representation of the second utterance, and such that the context rule is based on the semantic representation of the first response. A second response is generated for presentation to the user based on the second speech processing results. INDEPENDENT CLAIMS are included for the following:a computer-implemented method for processing and interpreting natural language; andnon-transitory computer-readable storage for processing and interpreting natural language. System for processing and interpreting natural language such as interpretations of user utterances. Advantageously, interpreting current utterances within the context of prior turns in a multi-turn interaction can help solve ambiguities without requiring additional user input, and can help avoid initiating an incorrect action based on an interpretation of an utterance in isolation when the correct action is determinable in the context of the entire multi-turn interaction. The processing is based on the context of the multi-turn interaction can allow more natural communication with a speech processing system and more robust dialog management without loss of context. The drawing shows a block diagram of the illustrative data flows between modules of a speech processing system during multi-turn dialog utterance processing using contextual information from previous turns. 102Microphone104Speaker206Context interpreter208Dialog manager212Context data store A system, comprising: a computer-readable memory that stores executable instructions and one or more processors coupled to said computer readable memory, wherein the one or more processors of the executable instructions programmed to at least: obtaining first audio data associated with the first utterance of the user, at least partially based on the first audio data to generate a first voice processing result, the first voice processing result comprises semantic expression the first utterance; based at least in part on the first voice processing result to generate a first response to the user presence, and the storing the context information, the context information comprises semantic expression of the first utterance and semantic expression of the first response, the second utterance associated with the user of the second audio data; using the second audio data, the context information and the context interpretation rule to generate a second voice processing result, at least one replacing the semantic expression of the first speech time slot value or attempted wherein the context interpretation rule with the semantics of expression of said second speech time slot value or attempted in at least one associated, and wherein the context rules are at least partly for presenting to the user a second response based on the second speech processing result generated based on the semantic expression of the first response, and is at least partially.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20140365222A1;Mobile system for telematics applications for processing natural language human-machine utterances, has processors for determining words of natural language utterance and determining request based on words and context information The system has processors for generating a context stack comprising context information that corresponds to prior utterances. The processors receive a natural language utterance associated with a command or request and determine words of the natural language utterance by performing speech recognition on the natural language utterance. The processors determine the command or request based on the words and the context information. The processors prompt a user associated with the natural language utterance. An INDEPENDENT CLAIM is also included for a computer-implemented method for processing natural language utterances. Mobile system for telematics applications for processing natural language human-machine utterances in environments. Uses include but are not limited to personal automobile environments, rented automobile environments, fleet automobile environments, two wheeled or open-air vehicle environments e.g. motorcycle environments and scooter environments, commercial long-haul and short haul lorry environments, delivery service vehicle environments, fleet service vehicle environments, industrial vehicle environments, agricultural and construction machinery environments, water-borne vehicle environments, aircraft environments, specialized military, law enforcement and emergency vehicle environments. The system uses travel service capability in conjunction with the remote ordering and payment capabilities and the dynamic interactive routing capability. The system allows an event manager to provide a multi-threaded environment so as to operate the system on multiple commands or questions from multiple user sessions without conflict and in an efficient manner, thus maintaining real-time response capabilities. The system is comfortable to use, and allows users to be comfortable with voice query language and learn how to optimize amount of information required with each step of a dialog using the interactive training. The drawing shows a schematic block diagram of a mobile system for processing natural language human-machine utterances. 10Mobile structure26Data interfaces28Telematics control unit30Control and device interface98Main speech-processing unit A system for processing natural language utterances where recognized words of the natural language utterances alone are insufficient to completely determine one or more commands or requests, the system including one or more processors executing one or more computer program modules which, when executed, cause the one or more processors to: generate a context stack comprising context information that corresponds to a plurality of prior utterances; receive a natural language utterance associated with a command or request; determine one or more words of the natural language utterance by performing speech recognition on the natural language utterance; and determine the command or request based on the one or more words and the context information.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160035348A1;Method for processing natural language queries in mobile environment, involves extracting natural language concept information from text and searching environment database containing information descriptive of objects The method involves extracting natural language concept information from text recognized by an automatic speech recognition engine. The text and the natural language concept information are used to assign to the speech input a query intent related to objects in the mobile environment. The environment database containing information descriptive of the objects is searched (305) in the mobile environment to determine corresponding search results. INDEPENDENT CLAIMS are included for the following:a computer program product for processing natural language queries in a mobile environment; andan apparatus for processing a natural language query related to objects in a mobile environment. Method for processing natural language queries in a mobile environment. The method enables faster and safer resolution of vehicle-related events using the speech enabled guide or wizard. The drawing shows a flowchart of a natural language query process. 301Obtaining speech input302Processing speech input303Processing results304Determining query intent305Searching environment database A method for processing natural language queries in a mobile environment employing at least one hardware implemented computer processor, the method comprising: extracting natural language concept information from text recognized by an automatic speech recognition (ASR) engine; using the text and the natural language concept information to assign to the speech input a query intent related to one or more objects in the mobile environment; and based on the query intent, the natural language concept information, and the recognition text, searching an environment database containing information descriptive of the one or more objects in the mobile environment to determine corresponding search results.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US8793138B2;Method for recognizing voice input commands by display device e.g. personal computer, involves recognizing voice input as voice command by voice recognizing unit and processing voice command, by another voice recognizing unit The method involves receiving (501) a voice input. A voice input is recognized by voice recognizing unit as a voice command. The voice command is processed (504), by another voice recognizing unit. A feature of the display device is controlled according to the voice command. The voice command is a natural language voice command, and the recognition of the voice command is provided to initiate a voice input command mode. The feature of the display device is controlled according to the voice command. An INDEPENDENT CLAIM is included for a display device. Method for recognizing voice input commands by display device (claimed) such as mobile telecommunications device, notebook computer, personal computer, tablet computing device, portable navigation device, portable video player and personal digital assistant (PDA). Since the voice command is processed by another voice recognizing unit, the user's voice command can be easily and efficiently recognized by the display device. Thus the features of the display device can be efficiently controlled. The occurrences of errors during natural language voice recognition process can be decreased by dedicating voice recognition unit to process user's natural language voice commands. The drawing shows a flow chart illustrating the process for recognizing voice input commands by display device. 501Step for receiving voice input502Step for determining whether input include voice command word included within list of preset voice commands503Step for processing the voice input on voice recognition unit504Step for processing voice input on another recognition unit A method of recognizing voice input commands by a display device, the method comprising: displaying a list of preset words that are available for receiving as input, wherein at least one of the preset words comprises a description for a natural language word that is available for input with the at least one of the preset words; receiving a first voice input comprising (i) a preset word that is a first portion of the first voice input and (ii) a natural language word that is a second portion of the first voice input; recognizing, by a first voice recognizing unit, the preset word that is the first portion of the first voice input as a preset voice command to be processed by the first voice recognizing unit; recognizing, by the first voice recognizing unit, the natural language word that is the second portion of the first voice input as a natural language voice command to be processed by a second voice recognizing unit; processing the preset voice command, by the first voice recognizing unit; processing the natural language voice command, by the second voice recognizing unit; and controlling a feature of the display device according to the processed preset voice command and the processed natural language voice command.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170070783A1;Computer implemented method for assisted viewing of audio video stream of notably sports contests, involves associating audio video stream index with audio video stream and receiving user natural language commands from user input device The computer implemented method involves associating an audio video stream index with audio video stream, where the audio video stream index is provided with a symbolic representation of events in the audio video stream. The user natural language commands are received from a user input device. The control signals are created from the user natural language commands by referencing the audio video stream index. The control signals are used to control a controllable audio video stream repository. The controlled audio video stream viewable on a primary display is produced. An INDEPENDENT CLAIM is included for an apparatus for assisted viewing of an audio video stream of notably sports contests. Computer implemented method for assisted viewing of an audio video stream of notably sports contests, such as basketball game and soccer game. The computer implemented method involves associating an audio video stream index with audio video stream, where the audio video stream index is provided with a symbolic representation of events in the audio video stream, and thus enables improving the viewing of the subject audio video stream by enabling the use of spoken or written commands. The drawing shows a schematic view of a relationship among real time, game clock time and game segments. A computer implemented method for assisted viewing of an audio video stream, comprising: an audio video stream source originating an audio video stream; associating an audio video stream index with said audio video stream, the audio video stream index comprising a symbolic representation of events within said audio video stream; a processor receiving user natural language commands from a user input device; said processor creating control signals from said user natural language commands by referencing said audio video stream index; said processor using said control signals to control a controllable audio video stream repository; and said processor producing from said controllable audio video stream repository a controlled audio video stream viewable on a primary display.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9495957B2;Mobile system for telematics applications for processing natural language human-machine utterances, has processors for determining words of natural language utterance and determining request based on words and context information The system has processors for generating a context stack comprising context information that corresponds to prior utterances. The processors receive a natural language utterance associated with a command or request and determine words of the natural language utterance by performing speech recognition on the natural language utterance. The processors determine the command or request based on the words and the context information. The processors prompt a user associated with the natural language utterance. An INDEPENDENT CLAIM is also included for a computer-implemented method for processing natural language utterances. Mobile system for telematics applications for processing natural language human-machine utterances in environments. Uses include but are not limited to personal automobile environments, rented automobile environments, fleet automobile environments, two wheeled or open-air vehicle environments e.g. motorcycle environments and scooter environments, commercial long-haul and short haul lorry environments, delivery service vehicle environments, fleet service vehicle environments, industrial vehicle environments, agricultural and construction machinery environments, water-borne vehicle environments, aircraft environments, specialized military, law enforcement and emergency vehicle environments. The system uses travel service capability in conjunction with the remote ordering and payment capabilities and the dynamic interactive routing capability. The system allows an event manager to provide a multi-threaded environment so as to operate the system on multiple commands or questions from multiple user sessions without conflict and in an efficient manner, thus maintaining real-time response capabilities. The system is comfortable to use, and allows users to be comfortable with voice query language and learn how to optimize amount of information required with each step of a dialog using the interactive training. The drawing shows a schematic block diagram of a mobile system for processing natural language human-machine utterances. 10Mobile structure26Data interfaces28Telematics control unit30Control and device interface98Main speech-processing unit A system for processing a natural language utterance, the system including one or more processors executing one or more computer program modules which, when executed, cause the one or more processors to: generate a context stack comprising context information that corresponds to a plurality of prior utterances, wherein the context stack includes a plurality of context entries; receive the natural language utterance, wherein the natural language utterance is associated with a command or is associated with a request; determine one or more words of the natural language utterance by performing speech recognition on the natural language utterance; identify, from among the plurality of context entries, one or more context entries that correspond to the one or more words, wherein the context information includes the one or more context entries, wherein identifying the one or more context entries comprises: comparing the plurality of context entries to the one or more words; generating, based on the comparison, one or more rank scores for individual context entries of the plurality of context entries; and identifying, based on the one or more rank scores, the one or more context entries from among the plurality of context entries; and  determine, based on the determined one or more words and the context information, the command or the request associated with the natural language utterance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KR2017126023A;Method for hybrid processing in natural language voice services environment, involves resolving multi-modal interaction at electronic device based on information contained in multiple messages received from virtual router The method involves detecting multi-modal interaction at an electronic device e.g. mobile phone. Multiple messages containing information related to the multi-modal interaction is communicated to a virtual router in communication with the electronic device. Multiple messages containing information related to intent of the multi-modal interaction is received by the electronic device. The multi-modal interaction at the electronic device is resolved based on the information contained in multiple messages received from the virtual router. INDEPENDENT CLAIMS are also included for the following:an electronic device for hybrid processing in a natural language voice services environmenta virtual router for hybrid processing in a natural language voice services environment. Method for hybrid processing in natural language voice services environment using an electronic device e.g. microphone, personal navigation device, mobile phone, Voice over Internet Protocol (VoIP) node, personal computer, media device, embedded device and server, and a virtual router (all claimed). The method enables performing hybrid processing in the natural language voice services environment using the electronic device and/or the virtual router without degrading performance in an efficient manner. The drawing shows a block diagram of a voice-enabled device for hybrid processing in a natural language voice services environment.100Voice-enabled device112Input devices120Automatic speech recognizer130Conversational language processor134Applications A hybrid processing method, wherein: the step multi-modal interaction which is the method for the hybrid processing at the natural language voice service environment; and detects at least one multi-modal interaction at the electronic device include the voice input received from at least first input apparatus and the non-voice input received from the second input apparatus, and the voice input includes the natural language speech (natural language utterance), and the non-voice input includes - the situation (context) for interpreting the natural language speech is expressed; - the step electronic device for delivering to the router device for communicating at least one message including the information about the voice input and non-voice input with the electronic device delivers at least one message through the messaging interface to the router device; and the information in which the step response receiving the response from the router device indicates the intention of the multi-modal interaction at the electronic device, and the electronic device comprises - at least one message is received through the messaging interface from the router device; and the step of doing the multi-modal interaction at the electronic device based on the response received from the router device with the resolution (resolving).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US8793136B2;Method for recognizing voice input commands by display device e.g. personal computer, involves recognizing voice input as voice command by voice recognizing unit and processing voice command, by another voice recognizing unit The method involves receiving (501) a voice input. A voice input is recognized by voice recognizing unit as a voice command. The voice command is processed (504), by another voice recognizing unit. A feature of the display device is controlled according to the voice command. The voice command is a natural language voice command, and the recognition of the voice command is provided to initiate a voice input command mode. The feature of the display device is controlled according to the voice command. An INDEPENDENT CLAIM is included for a display device. Method for recognizing voice input commands by display device (claimed) such as mobile telecommunications device, notebook computer, personal computer, tablet computing device, portable navigation device, portable video player and personal digital assistant (PDA). Since the voice command is processed by another voice recognizing unit, the user's voice command can be easily and efficiently recognized by the display device. Thus the features of the display device can be efficiently controlled. The occurrences of errors during natural language voice recognition process can be decreased by dedicating voice recognition unit to process user's natural language voice commands. The drawing shows a flow chart illustrating the process for recognizing voice input commands by display device. 501Step for receiving voice input502Step for determining whether input include voice command word included within list of preset voice commands503Step for processing the voice input on voice recognition unit504Step for processing voice input on another recognition unit A method of recognizing voice input commands by a display device, the method comprising: receiving a first voice input comprising (i) a preset word that is a first portion of the first voice input and (ii) a natural language word that is a second portion of the first voice input; recognizing, by a first voice recognizing unit, the preset word that is the first portion of the first voice input as a preset voice command to be processed by the first voice recognizing unit; recognizing, by the first voice recognizing unit, the natural language word that is the second portion of the first voice input as a natural language voice command to be processed by a second voice recognizing unit; processing the preset voice command, by the first voice recognizing unit; processing the natural language voice command, by the second voice recognizing unit; and controlling a feature of the display device according to the processed preset voice command and the processed natural language voice command, wherein the first voice input is a single sentence comprising the preset word and the natural language word.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9229681B2;Method for recognizing voice input commands by display device e.g. personal computer, involves recognizing voice input as voice command by voice recognizing unit and processing voice command, by another voice recognizing unit The method involves receiving (501) a voice input. A voice input is recognized by voice recognizing unit as a voice command. The voice command is processed (504), by another voice recognizing unit. A feature of the display device is controlled according to the voice command. The voice command is a natural language voice command, and the recognition of the voice command is provided to initiate a voice input command mode. The feature of the display device is controlled according to the voice command. An INDEPENDENT CLAIM is included for a display device. Method for recognizing voice input commands by display device (claimed) such as mobile telecommunications device, notebook computer, personal computer, tablet computing device, portable navigation device, portable video player and personal digital assistant (PDA). Since the voice command is processed by another voice recognizing unit, the user's voice command can be easily and efficiently recognized by the display device. Thus the features of the display device can be efficiently controlled. The occurrences of errors during natural language voice recognition process can be decreased by dedicating voice recognition unit to process user's natural language voice commands. The drawing shows a flow chart illustrating the process for recognizing voice input commands by display device. 501Step for receiving voice input502Step for determining whether input include voice command word included within list of preset voice commands503Step for processing the voice input on voice recognition unit504Step for processing voice input on another recognition unit A method of recognizing voice input commands by a display device, the method comprising: displaying a list of preset words that are available for receiving as input, at least one of the preset words comprising a description for a natural language word that is available for input with the at least one of the preset words; after displaying the list of preset words, receiving a first voice input comprising (i) a preset word that is a first portion of the first voice input and (ii) a natural language word that is a second portion of the first voice input; recognizing the preset word that is the first portion of the first voice input as a preset voice command to be processed by a first voice recognizing unit; recognizing the natural language word that is the second portion of the first voice input as a natural language voice command to be processed by a second voice recognizing unit; processing, by the first voice recognizing unit, the preset voice command; processing, by the second voice recognizing unit, the natural language voice command; and controlling a feature of the display device according to the processed preset voice command and the processed natural language voice command.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170133010A1;Computer-implemented method for recognizing e.g. names of persons in speech, involves substituting portion of recognition results of second automatic speech recognition performed on second utterance by user with selected proper name The method involves receiving a first and second dataset associated with the recognition results of a first and second automatic speech recognition operation performed upon a first and second utterance by a user. A corpus of relevant proper names is determined based on the data. A portion of the second utterance corresponding to proper name based on the results is determined. A grammar-based automatic speech recognition system is caused to select a proper name. The portion of the recognition results of the second speech recognition is substituted with the selected proper name. INDEPENDENT CLAIMS are included for the following:a computer system; anda non-transitory computer readable medium storing program for recognizing proper names in speech. Computer-implemented method for recognizing proper names such as names of persons, streets, cities, businesses, landmarks, songs, videos or other entities in speech. The method enables high accuracy recognition, understanding of freely spoken utterances which contain proper names and other similar entities. The temporal smoothing of the audio input eliminates abrupt audio transitions which are falsely matched as fricative phonemes. Proper selection of the span extent improves secondary decoding and also allows coarticulation effects in selecting phone models during the decoding process. The drawing shows a graphical depiction of generated grammar. A computer-implemented method for recognizing and understanding spoken commands that include one or more proper name entities, comprising: receiving an utterance from a user; performing primary automatic speech recognition (ASR) processing upon said utterance with a primary automatic speech recognizer to output a dataset comprising at least a sequence of nominal transcribed words and putative start and end times for each nominal transcribed word within said utterance; performing understanding processing upon said dataset with a natural language understanding (NLU) processor to generate and augment the dataset with a nominal meaning for the utterance and to determine putative presence and type of one or more spoken proper name entities within said utterance, wherein a contiguous section of audio within said utterance corresponding to each putative proper name entity, as determined from said start and end times of the words of the putative proper name entity as transcribed by the primary automatic speech recognizer, comprises an acoustic span; performing secondary automatic speech recognition (ASR) processing upon each said acoustic span with a secondary automatic speech recognizer, in each instance said secondary automatic speech recognizer specialized to process a given putative type of acoustic span to generate a nominal correct transcription and associated meaning for each said acoustic span; substituting the nominal correct transcription and associated meaning obtained from each secondary recognition as appropriate within the dataset to revise the results of the primary automatic speech recognizer and natural language understanding processor and to create a plurality of complete transcriptions and associated meanings; preparing a complete hypothesis ranking grammar comprised of said plurality of complete transcriptions and decoding the utterance against said complete hypothesis ranking grammar to determine an acoustic confidence score for each complete transcription; determining, for each acoustic span of each complete transcription, an NLU confidence score for each transcription of each acoustic span; normalizing said NLU confidence scores across the plurality of complete transcriptions to determine a normalized NLU confidence score of each complete transcription; combining said acoustic confidence score and NLU confidence score of each complete transcription to generate a final confidence score that each complete transcription and associated meaning is correct, which is used to rank the plurality of aforesaid complete transcriptions and associated meanings; and outputting a ranked list of complete transcriptions and associated meanings for the entire utterance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US9818401B2;Computer-implemented method for recognizing e.g. names of persons in speech, involves substituting portion of recognition results of second automatic speech recognition performed on second utterance by user with selected proper name The method involves receiving a first and second dataset associated with the recognition results of a first and second automatic speech recognition operation performed upon a first and second utterance by a user. A corpus of relevant proper names is determined based on the data. A portion of the second utterance corresponding to proper name based on the results is determined. A grammar-based automatic speech recognition system is caused to select a proper name. The portion of the recognition results of the second speech recognition is substituted with the selected proper name. INDEPENDENT CLAIMS are included for the following:a computer system; anda non-transitory computer readable medium storing program for recognizing proper names in speech. Computer-implemented method for recognizing proper names such as names of persons, streets, cities, businesses, landmarks, songs, videos or other entities in speech. The method enables high accuracy recognition, understanding of freely spoken utterances which contain proper names and other similar entities. The temporal smoothing of the audio input eliminates abrupt audio transitions which are falsely matched as fricative phonemes. Proper selection of the span extent improves secondary decoding and also allows coarticulation effects in selecting phone models during the decoding process. The drawing shows a graphical depiction of generated grammar. A computer-implemented method for recognizing and understanding spoken commands that include one or more proper name entities, comprising: receiving an utterance from a user; performing primary automatic speech recognition (ASR) processing upon said utterance with a primary automatic speech recognizer to output a dataset comprising at least a sequence of nominal transcribed words and putative start and end times for each nominal transcribed word within said utterance; performing understanding processing upon said dataset with a natural language understanding (NLU) processor to generate and augment the dataset with a nominal meaning for the utterance and to determine putative presence and type of one or more spoken proper name entities within said utterance, wherein a contiguous section of audio within said utterance corresponding to each putative proper name entity, as determined from said start and end times of the words of the putative proper name entity as transcribed by the primary automatic speech recognizer, comprises an acoustic span; performing secondary automatic speech recognition (ASR) processing upon each said acoustic span with a secondary automatic speech recognizer, in each instance said secondary automatic speech recognizer specialized to process a given putative type of acoustic span to generate a nominal correct transcription and associated meaning for each said acoustic span; substituting the nominal correct transcription and associated meaning obtained from each secondary recognition as appropriate within the dataset to revise the results of the primary automatic speech recognizer and natural language understanding processor and to create a plurality of complete transcriptions and associated meanings; preparing a complete hypothesis ranking grammar comprised of said plurality of complete transcriptions and decoding the utterance against said complete hypothesis ranking grammar to determine an acoustic confidence score for each complete transcription; determining, for each acoustic span of each complete transcription, an NLU confidence score for each transcription of each acoustic span; normalizing said NLU confidence scores across the plurality of complete transcriptions to determine a normalized NLU confidence score of each complete transcription; combining said acoustic confidence score and NLU confidence score of each complete transcription to generate a final confidence score that each complete transcription and associated meaning is correct, which is used to rank the plurality of aforesaid complete transcriptions and associated meanings; and outputting a ranked list of complete transcriptions and associated meanings for the entire utterance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170278514A1;Method for performing natural language understanding, involves identifying musician corresponding to word, receiving indication that musical work is associated with musician from base, and causing audio corresponding to work to be output The method involves processing a text using a log linear model (LLM) classifier to determine a score representing a likelihood that a word corresponds to a person category in a hierarchical arrangement of entity types. A determination is made to check that another score is higher than the former score. A musician is identified corresponding to the word using a musician category and a knowledge base. An indication that a musical work is associated with the musician is received from the knowledge base. Audio (11) is caused corresponding to the musical work to be output. An INDEPENDENT CLAIM is also included for a system for performing a natural language understanding. Method for performing a natural language understanding. The method enables utilizing an automatic speech recognition (ASR) process that compares input audio data with models for sounds and sequences of sounds to identify words that match a sequence of sounds spoken in an utterance of the audio data. The method enables utilizing language information to adjust an acoustic score by considering what sounds and/or words are used in context with each other, thus improving the likelihood that the ASR process outputs speech results that make sense grammatically. The drawing shows a schematic view of a system for configuring and operating the system to parse incoming queries. 11Audio100System performing natural language understanding110Device120Servers199Network A computer-implemented method comprising: receiving text corresponding to an utterance; creating first data indicating that a first word of the text is a noun; processing the text and the first data by a conditional random field classifier to determine that the text corresponds to a command and that the first word corresponds to an entity in a knowledge base; processing the text using a first log linear model (LLM) classifier to determine: a first score representing a likelihood that the text corresponds to a play media command category in a hierarchical arrangement of commands, a second score representing a likelihood that the text corresponds to a play song command in the hierarchical arrangement of commands, wherein the play song command is a command within the play media command category, and a third score representing a likelihood that the text corresponds to a play book text command in the hierarchical arrangement of commands, wherein the play book text command is a command within the play media command category;  determining that the second score is higher than the first score and the third score; processing the text using a second LLM classifier to determine: a fourth score representing a likelihood that the first word corresponds to a person category in a hierarchical arrangement of entity types, and a fifth score representing a likelihood that the first word corresponds to a musician category in the hierarchical arrangement of entity types, where the musician category is a subset of the person category;  determining that the fifth score is higher than the fourth score; identifying, using the musician category and the knowledge base, a musician corresponding to the first word; receiving, from the knowledge base, an indication that a first musical work is associated with the musician; and causing audio corresponding to the first musical work to be output.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KR1798828B1;Method for hybrid processing in natural language voice services environment, involves resolving multi-modal interaction at electronic device based on information contained in multiple messages received from virtual router The method involves detecting multi-modal interaction at an electronic device e.g. mobile phone. Multiple messages containing information related to the multi-modal interaction is communicated to a virtual router in communication with the electronic device. Multiple messages containing information related to intent of the multi-modal interaction is received by the electronic device. The multi-modal interaction at the electronic device is resolved based on the information contained in multiple messages received from the virtual router. INDEPENDENT CLAIMS are also included for the following:an electronic device for hybrid processing in a natural language voice services environmenta virtual router for hybrid processing in a natural language voice services environment. Method for hybrid processing in natural language voice services environment using an electronic device e.g. microphone, personal navigation device, mobile phone, Voice over Internet Protocol (VoIP) node, personal computer, media device, embedded device and server, and a virtual router (all claimed). The method enables performing hybrid processing in the natural language voice services environment using the electronic device and/or the virtual router without degrading performance in an efficient manner. The drawing shows a block diagram of a voice-enabled device for hybrid processing in a natural language voice services environment.100Voice-enabled device112Input devices120Automatic speech recognizer130Conversational language processor134Applications In the natural language voice service environment, it is the virtual router for the hybrid processing and the virtual router delivers to multiple messages including the audio input which the first is encoded corresponding to the natural language speech which is included in the multi-modal interaction of the multiple individual electronic devices and is captured by the first input apparatus and the audio input which the second corresponding to the natural language speech captured by the second input apparatus is encoded are received; - the second number of the audio capability of the audio input which the first value of the audio capability of the audio input which the first is encoded and the second is encoded is decided to analyze the audio input which multiple inside of the messages is encoded and one message providing the sample which is most clear (cleanest) of the natural language speech be determined among multiple messages and analyze the above-mentioned encoded audio input and the audio input which the first is encoded or the audio input which the second is encoded is chosen based on the first value and the second number and the intention of the natural language input is obtained based on the encoded audio input which is selected from the audio input which the first is encoded or the audio input which the second is encoded; and the server communicating at least one message including the above-mentioned encoded audio input providing the sample which is most clear with the virtual router. At least one message tells about at least one message including - it is delivered to the server through the messaging interface; at least one message including the information about the intention of the multi-modal interaction is received through the messaging interface from the server; and the information about the intention of the multi-modal interaction among multiple electronic devices in at least one electronic device with the turnaround (return).A virtual router, wherein: the virtual router is formed, and at least one electronic device solves the multi-modal interaction among electronic devices based on the information about the intention of the multi-modal interaction; and comprises to return at least one message delivering the request to the electronic device for communicating with the virtual router processing the request.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KR2017091797A;Method for hybrid processing in natural language voice services environment, involves resolving multi-modal interaction at electronic device based on information contained in multiple messages received from virtual router The method involves detecting multi-modal interaction at an electronic device e.g. mobile phone. Multiple messages containing information related to the multi-modal interaction is communicated to a virtual router in communication with the electronic device. Multiple messages containing information related to intent of the multi-modal interaction is received by the electronic device. The multi-modal interaction at the electronic device is resolved based on the information contained in multiple messages received from the virtual router. INDEPENDENT CLAIMS are also included for the following:an electronic device for hybrid processing in a natural language voice services environmenta virtual router for hybrid processing in a natural language voice services environment. Method for hybrid processing in natural language voice services environment using an electronic device e.g. microphone, personal navigation device, mobile phone, Voice over Internet Protocol (VoIP) node, personal computer, media device, embedded device and server, and a virtual router (all claimed). The method enables performing hybrid processing in the natural language voice services environment using the electronic device and/or the virtual router without degrading performance in an efficient manner. The drawing shows a block diagram of a voice-enabled device for hybrid processing in a natural language voice services environment.100Voice-enabled device112Input devices120Automatic speech recognizer130Conversational language processor134Applications In the natural language voice service environment, it is the virtual router for the hybrid processing and the virtual router delivers to multiple messages including the audio input which the first is encoded corresponding to the natural language speech which is included in the multi-modal interaction of the multiple individual electronic devices and is captured by the first input apparatus and the audio input which the second corresponding to the natural language speech captured by the second input apparatus is encoded are received; - the second number of the audio capability of the audio input which the first value of the audio capability of the audio input which the first is encoded and the second is encoded is decided to analyze the audio input which multiple inside of the messages is encoded and one message providing the sample which is most clear (cleanest) of the natural language speech be determined among multiple messages and analyze the above-mentioned encoded audio input and the audio input which the first is encoded or the audio input which the second is encoded is chosen based on the first value and the second number and the intention of the natural language input is obtained based on the encoded audio input which is selected from the audio input which the first is encoded or the audio input which the second is encoded; and the server communicating at least one message including the above-mentioned encoded audio input providing the sample which is most clear with the virtual router. At least one message tells about at least one message including - it is delivered to the server through the messaging interface; at least one message including the information about the intention of the multi-modal interaction is received through the messaging interface from the server; and the information about the intention of the multi-modal interaction among multiple electronic devices in at least one electronic device with the turnaround (return).A virtual router, wherein: the virtual router is formed, and at least one electronic device solves the multi-modal interaction among electronic devices based on the information about the intention of the multi-modal interaction; and comprises to return at least one message delivering the request to the electronic device for communicating with the virtual router processing the request.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170221482A1;Multi-modal device interactions processing method for electronic devices in natural language voice services environment, involves routing request to electronic devices based on determined intent of multi-modal device interaction The method involves detecting a multi-modal device interaction. Context information related to the device interaction is extracted, where the extracted context information includes context related to a non-voice interaction. The context related to the non-voice interaction and a natural language utterance are combined. Intent of the device interaction is determined based on the combination of context related to the non-voice interaction and the natural language utterance. A request is routed to electronic devices (105) based on the determined intent of the device interaction. An INDEPENDENT CLAIM is also included for a system for processing multi-modal device interactions in a natural language voice services environment. Method for processing multi-modal device interactions between electronic devices in a natural language voice services environment. Uses include but are not limited to microphone, telematics device, personal navigation device, mobile phone, voice over Internet protocol (VoIP) node, personal computer, media device, embedded device, and server. The method enables determining the intent of the multi-modal device interaction based on the combination of context related to the non-voice interaction and the natural language utterance, so that the interaction with electronic devices is efficient, thus avoiding difficulties in utilizing the functionality of the devices. The method enables a voice-click module to enable a user to interact with the electronic devices in an intuitive and free-form manner, so that the user can provide various types of information to a system while seeking to initiate action, retrieve information and request content or services available in the system. The drawing shows a block diagram of a system for processing multi-modal device interactions in a natural language voice services environment.105Electronic devices105bNon-voice input device110Automatic speech recognizer120Conversational language processor160Data repositories A method for processing one or more multi-modal device interactions in a natural language voice services environment that includes one or more electronic devices, comprising: detecting at least one multi-modal device interaction, wherein the multi-modal device interaction includes a non-voice interaction with at least one of the electronic devices or an application associated with at least one of the electronic devices, and wherein the multi-modal device interaction further includes at least one natural language utterance relating to the non-voice interaction; extracting context information relating to the multi-modal device interaction, wherein the extracted context information includes context relating to the non-voice interaction, and wherein the extracted context information further include context relating to the natural language utterance; combining the context relating to the non-voice interaction and the context relating to the natural language utterance; determining an intent of the multi-modal device interaction based on the combined context relating to the non-voice interaction and the natural language utterance; and routing at least one request to one or more of the electronic devices based on the determined intent of the multi-modal device interaction.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20140249822A1;Multi-modal device interactions processing method for electronic devices in natural language voice services environment, involves routing request to electronic devices based on determined intent of multi-modal device interaction The method involves detecting a multi-modal device interaction. Context information related to the device interaction is extracted, where the extracted context information includes context related to a non-voice interaction. The context related to the non-voice interaction and a natural language utterance are combined. Intent of the device interaction is determined based on the combination of context related to the non-voice interaction and the natural language utterance. A request is routed to electronic devices (105) based on the determined intent of the device interaction. An INDEPENDENT CLAIM is also included for a system for processing multi-modal device interactions in a natural language voice services environment. Method for processing multi-modal device interactions between electronic devices in a natural language voice services environment. Uses include but are not limited to microphone, telematics device, personal navigation device, mobile phone, voice over Internet protocol (VoIP) node, personal computer, media device, embedded device, and server. The method enables determining the intent of the multi-modal device interaction based on the combination of context related to the non-voice interaction and the natural language utterance, so that the interaction with electronic devices is efficient, thus avoiding difficulties in utilizing the functionality of the devices. The method enables a voice-click module to enable a user to interact with the electronic devices in an intuitive and free-form manner, so that the user can provide various types of information to a system while seeking to initiate action, retrieve information and request content or services available in the system. The drawing shows a block diagram of a system for processing multi-modal device interactions in a natural language voice services environment.105Electronic devices105bNon-voice input device110Automatic speech recognizer120Conversational language processor160Data repositories A method for processing one or more multi-modal device interactions in a natural language voice services environment that includes one or more electronic devices, comprising: detecting at least one multi-modal device interaction, wherein the multi-modal device interaction includes a non-voice interaction with at least one of the electronic devices or an application associated with at least one of the electronic devices, and wherein the multi-modal device interaction further includes at least one natural language utterance relating to the non-voice interaction; extracting context information relating to the multi-modal device interaction, wherein the extracted context information includes context relating to the non-voice interaction, and wherein the extracted context information further include context relating to the natural language utterance; combining the context relating to the non-voice interaction and the context relating to the natural language utterance; determining an intent of the multi-modal device interaction based on the combined context relating to the non-voice interaction and the natural language utterance; and routing at least one request to one or more of the electronic devices based on the determined intent of the multi-modal device interaction.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170116989A1;Method for assisting user with desired tasks within domain for airline information system, involves performing domain-specific task in accordance with intention of user by executable generic task reasoning module and run-time specification The method involves determining intention of a user with respect to desired tasks from a verbal language input and one of a set of different kinds of non-verbal input by using an executable generic language understanding module and a run-time specification comprising a model configured to a specific field of use. A domain-specific task is performed in accordance with the intention of the user by using an executable generic task reasoning module (150) and run-time specification comprising task flow configured to the specific field of use. An INDEPENDENT CLAIM is also included for a virtual personal assistant platform for assisting a user with desired tasks within a domain. Method for assisting a user with desired tasks within a domain by providing a virtual personal assistant platform using a computer system (all claimed) for an airline information system for dialog-driven interactive applications. Uses include but are not limited to assist the user in information discovery, rating of articles, desktop and web search, document management and team collaboration by using a workstation, smart phone, cell phone, personal digital assistant and a mobile communication device. The method enables advancing artificial intelligence and speech recognition technology to support fast and efficient development of virtual personal assistants that understand the user's spoken and/or written input, perform tasks and adapt to user preferences over time for corresponding domain-specific models without necessity of revising or rewriting source code of the generic platform and with little or no programmer-level involvement in creating the domain-specific models. The drawing shows a block diagram of a virtual personal assistant system. 130Sentence-level understanding module140Interpreter150Executable generic task reasoning module191Voice/speech synthesizer192Display 29. A method for assisting a user with one or more desired tasks within a domain, the method comprising: receiving, by a computing system comprising one or more computing devices, a verbal language input and at least one of a plurality of different kinds of non-verbal input from the user; determining, by the computing system, from the verbal language input and the at least one of a plurality of different kinds of non-verbal input, an intention of the user with respect to the one or more desired tasks, by an executable generic language understanding module and a run-time specification comprising a model configured to a specific field of use; and performing, by the computing system, a domain-specific task in accordance with the intention of the user, by an executable generic task reasoning module and a run-time specification comprising a task flow configured to the specific field of use.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170221476A1;Computer implemented method for constructing natural language model for processing speech, involves converting speech input into text using speech recognizer based on natural language model, and classifying text using topic classifier The method involves executing a topic-specific query on a database. Relevant documents are retrieved from the database in response to executing the query. A refined topic-specific text corpus is formed based on the relevant documents. A topic classifier is trained using the refined topic specific text corpus. A natural language model is generated based on the refined topic specific text corpus. A speech input is received. The speech input is converted into a text using a speech recognizer based on the natural language model. The text is classified using the topic classifier. Computer implemented method for constructing a natural language model for processing speech. The method enables using the same topic-specific corpus for performing each of training the topic classifier and generation of topic-specific language model for the speech recognizer, and subsequently utilizing each of the speech recognizer and the topic classifier in conjunction to process the speech input, so that improved performance cam be attained in processing first language based input utilizing each of first language processing (LP) application, a first language model and a second LP application. The method enables using identified topics to automatically extract relevant documents, human resource (HR) records or other content and displayed to the user during conversation as aide memoire or to ease access to relevant information to inform the conversation. The drawing shows a block diagram of a system for constructing a natural language model. 604Generating module606Constructing module700System for constructing natural language model702Natural language processing application A computer implemented method of constructing a language model for processing speech, the computer implemented method comprising: training a topic classifier using at least one topic specific text corpus; identifying a set of keywords based on the training, wherein the set of keywords correspond to at least one topic; executing at least one topic-specific query on a database, wherein the at least one topic specific query comprises the set of keywords; retrieving relevant documents from the database based on the executing; forming at least one refined topic-specific text corpus based on the relevant documents; training the topic classifier using the at least one refined topic specific text corpus; generating a natural language model based on the at least one refined topic specific text corpus; receiving a speech input; converting the speech input into a text using a speech recognizer based on the natural language model; and classifying the text using the topic classifier.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170161829A1;Distributed computer system for executing trades at mid-market prices, has portfolio/order management system operatively communicating with client, where management system and custodian provide bond buy and bond sell assistance to client The system has a server (116) including a processor and a memory and operatively coupled to a client (101,114, 115). A third party pricing service is operatively communicated with the server. A custodian (102) operatively communicates with the client. A portfolio/order management system operatively communicates with the client. The portfolio/order management system and the custodian provide bond buy and bond sell assistance to the client. A cryptographic network server (113) is operatively connected and communicates with the client. The client includes a voice recognition server, a natural language processor and a natural language to database query engine. An INDEPENDENT CLAIM is also included for a method for executing trades at mid-market prices through a distributed computer system. Distributed computer system for executing trades at mid-market prices. The system ensures enhanced user experience configuration to provide capability for buyers and sellers to use a multi-modal interface including speech recognition to submit complex trading strategies or bond selection criteria, which are processed and converted to potential orders. The drawing shows a schematic block diagram of a cryptographically secure electronic peer-to-peer bond-trading network for executing trades at mid-market prices. 101,114, 115Clients102Custodian113Cryptographic network server116Server A distributed computer system comprising a cryptographically secure electronic peer-to-peer bond-trading network for executing trades at mid-market prices, the computer system comprising: at least one client including a processer and memory, at least one server and an encryption engine; a server including a processor and memory and operatively coupled to the at least one client; a third party pricing service operatively communicating with the server; a custodian operatively communicating with the at least one client; and a portfolio/order management system operatively communicating with the at least one client, the portfolio/order management system and the custodian providing at least one of bond buy and bond sell assistance to the at least one client.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20170186426A1;Device for predictive human control of e.g. multifunction peripheral, has processor that compares instruction data stored in memory from previous user session with instruction data received from subsequent user session for common user The device has a language parser that extracts instruction data from the processed natural language input. A memory stores user input data corresponding to processed natural language input and store instruction data associated with identification data. An output unit communicates output instruction to control associated device. A processor (202) compares instruction data stored in memory from previous user session with instruction data received from subsequent user session for common user and generates output instruction based on comparison. The device has processor that processes natural language input received through user interface for discrete user session and identifies users associated with each user session so as to associate processed natural language input with identification data corresponding to each user. INDEPENDENT CLAIMS are included for the following:a predictive device control method; anda predictive device control system. Device for predictive human control of device such as document processing device e.g. multifunction peripheral (MFP). Can also be used in scanner, printer, copier, plotter and fax machine. Since the ongoing man or machine interactions reveal much about a user's preferences and habits, the predictive control of devices subjected to a user's confirmation of machine-proposed activity is facilitated. Thus, the user's experience in a future man or machine interfacing is improved in an easy and efficient manner to provide better user experience. The document processing operation is commenced with the user's instruction, so that the MFP provides processing time information to the user in a user-friendly manner. The drawing shows a block diagram of the document processing device. 202Processor210Network interface212Data bus216Storage interface218Network interface A device comprising: a user interface; a processor configured to process natural language input received via the user interface for each of a plurality of discrete user sessions, the processor configured to identify users associated with each of the plurality of discrete user sessions, and the processor configured to associate processed natural language input with identification data corresponding to each user associated therewith;  a language parser configured to extract instruction data from the processed natural language input; a memory configured to store user input data corresponding to processed natural language input, the memory configured to store instruction data associated with identification data; and an output configured to communicate an output instruction to control an associated device, wherein the processor is further configured to compare instruction data stored in the memory from a prior user session with instruction data received from a subsequent user session for a common user, and wherein the processor is further configured to generate the output instruction based on the comparison.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
US20160379638A1;Computer-implemented method for processing whispered utterance and responding in whispered synthesized speech, involves determining content responding to query based on input utterance being whispered and causing content to be output The method involves receiving (602) input audio data comprising an input utterance. The input audio data with trained model is processed to determine (604) that the input utterance is whispered. The automatic speech recognition (ASR) is performed (610) on the input audio data to determine input text corresponding to the input utterance. The natural language understanding processing is performed on the input text to identify a query. The content responding to the query is determined based on the input utterance being whispered. The content is caused to be output. An INDEPENDENT CLAIM is included for computing system for processing whispered utterance and responding in whispered synthesized speech. Computer-implemented method for processing whispered utterance and responding in whispered synthesized speech. The language information is used to adjust the acoustic score by considering what sounds and/or words are used in context with each other, thereby improving the that the ASR process will output speech results that make sense grammatically. The TTS module can revise/update the contents of the TTS storage based on feedback of the results of TTS processing, thus enabling the TTS module to improve speech recognition. The drawing shows a flow diagrams illustrating matching system output to an input speech quality. 602Step for receiving input audio data604Step for determining speech quality606Step for determining indicator of speech610Step for performing ASR612Step for sending utterance text to command processor A computer-implemented method for processing a whispered utterance and responding in whispered synthesized speech, the method comprising: receiving input audio data comprising an input utterance; processing the input audio data with at least one trained model to determine that the input utterance was whispered; performing automatic speech recognition (ASR) on the input audio data to determine input text corresponding to the input utterance; performing natural language understanding processing on the input text to identify a query; determining content responding to the query based on the input utterance being whispered; and causing the content to be output.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
